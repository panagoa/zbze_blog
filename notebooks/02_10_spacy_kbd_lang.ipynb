{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gzip\n",
    "from spacy.language import Language\n",
    "from spacy.lang.char_classes import LIST_ELLIPSES, LIST_CURRENCY, LIST_QUOTES, LIST_PUNCT\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import registry, compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "\n",
    "kbd_vocab = Vocab().from_bytes(gzip.open('../data/processed/spacy/kbd_with_vectors.bin.gz', 'rb').read())\n",
    "\n",
    "@registry.languages(\"kbd\")\n",
    "class KbdLanguage(Language):\n",
    "    lang = \"kbd\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.vocab = kbd_vocab\n",
    "        \n",
    "        # Настройка регулярных выражений для токенизатора\n",
    "        infix_re = compile_infix_regex(LIST_ELLIPSES + LIST_CURRENCY + LIST_QUOTES + LIST_PUNCT)\n",
    "        prefix_re = compile_prefix_regex(self.Defaults.prefixes)\n",
    "        suffix_re = compile_suffix_regex(self.Defaults.suffixes)\n",
    "\n",
    "        # Настройка токенизатора для нового языка\n",
    "        self.tokenizer = Tokenizer(\n",
    "            self.vocab,\n",
    "            prefix_search=prefix_re.search,\n",
    "            suffix_search=suffix_re.search,\n",
    "            infix_finditer=infix_re.finditer,\n",
    "            token_match=None  # Здесь можно добавить собственные правила для совпадения токенов\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T21:39:13.081844Z",
     "start_time": "2024-02-22T21:39:09.631790Z"
    }
   },
   "id": "c5ada8fa73559b27",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "from spacy.tokens import Token\n",
    "\n",
    "# Регистрируем кастомные атрибуты для токенов Spacy \n",
    "Token.set_extension('custom_prefix', default=None, force=True)\n",
    "Token.set_extension('custom_suffix', default=None, force=True)\n",
    "Token.set_extension('custom_stem', default=None, force=True)\n",
    "\n",
    "from tokenizers import Tokenizer as HFTokenizer\n",
    "\n",
    "tokenizer_uni = HFTokenizer.from_file('../data/processed/tokenizer/words_unigram_5000.tokenizer.json')\n",
    "\n",
    "@Language.factory(\"custom_prefix_suffix\")\n",
    "def create_custom_prefix_suffix_component(nlp, name):\n",
    "    def custom_prefix_suffix(doc):\n",
    "        for _token in doc:\n",
    "            word_tokens = tokenizer_uni.encode(_token.text).tokens\n",
    "            _token._.custom_prefix = word_tokens[0] if len(word_tokens) > 0 else None\n",
    "            _token._.custom_suffix = word_tokens[-1] if len(word_tokens) > 1 else None\n",
    "            _token._.custom_stem = ''.join(word_tokens[1:-1]) if len(word_tokens) > 2 else None\n",
    "\n",
    "        return doc\n",
    "    return custom_prefix_suffix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T21:39:14.820086Z",
     "start_time": "2024-02-22T21:39:14.813650Z"
    }
   },
   "id": "b1264845f8e2607f",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "verb_suffixes = [\"ыж\", \"ын\", \"ащ\"]  # Пример: список окончаний глаголов\n",
    "\n",
    "@Language.factory(\"simple_pos_tagger\")\n",
    "def create_simple_pos_tagger(nlp, name):\n",
    "    def simple_pos_tagger(doc):\n",
    "        # Пример простого правила: присваиваем тег \"NOUN\" (существительное), если слово заканчивается на \"ность\"\n",
    "        for token in doc:\n",
    "            for suffix in verb_suffixes:\n",
    "                if token.text.endswith(suffix):\n",
    "                    token.pos_ = \"VERB\"  # Глагол\n",
    "                    break\n",
    "            \n",
    "            # Добавьте здесь другие правила\n",
    "            else:\n",
    "                token.pos_ = \"X\"  # Неизвестная часть речи\n",
    "        return doc\n",
    "    return simple_pos_tagger"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T21:39:15.611382Z",
     "start_time": "2024-02-22T21:39:15.590372Z"
    }
   },
   "id": "b3819f9ea16485ea",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from spacy.pipeline import Lemmatizer\n",
    "\n",
    "LEMMA_INDEX = {\n",
    "    \"къиcтхыкIащ\": \"къитхыкIын\",\n",
    "    \"къритхыкIынущ\": \"къитхыкIын\",\n",
    "}\n",
    "\n",
    "# искдючения из правил, для которых не будет применятся LEMMA_RULES\n",
    "LEMMA_EXC = {\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "LEMMA_RULES = {\n",
    "    \"verbs\": [\n",
    "        [\"ыж\", \"ыжын\"],  # Удалить \"ать\" в конце глаголов\n",
    "    ],\n",
    "    \"nouns\": [\n",
    "        # Правила для существительных\n",
    "    ]\n",
    "    # Добавьте другие правила по аналогии\n",
    "}\n",
    "\n",
    "\n",
    "class KbdLanguageLemmatizer(Lemmatizer):\n",
    "    def __init__(self, vocab, name=\"kbd_lemmatizer\", mode=\"rule\"):\n",
    "        # Обязательно вызываем конструктор базового класса\n",
    "        super().__init__(vocab, model=None, name=name, mode=mode)\n",
    "        self.lemma_index = LEMMA_INDEX\n",
    "        self.lemma_exc = LEMMA_EXC\n",
    "        self.lemma_rules = LEMMA_RULES\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        # Пройдемся по всем токенам в документе и присвоим леммы\n",
    "        for token in doc:\n",
    "            lemma = self.lemmatize(token)  # Используйте здесь свою логику лемматизации\n",
    "            token.lemma_ = lemma[0] if lemma else token.text  # Присвоим лемму токену\n",
    "        return doc\n",
    "    \n",
    "    def rule_lemmatize(self, token):\n",
    "        token_text = token.text\n",
    "\n",
    "        # поиск в индексе\n",
    "        if token.text in self.lemma_index:\n",
    "            return [self.lemma_index[token_text]]\n",
    "\n",
    "        # исключения    \n",
    "        if token.text in self.lemma_exc:\n",
    "            return None\n",
    "        \n",
    "        if token.pos_ == \"VERB\":\n",
    "            # замена окончания\n",
    "            for old, new in self.lemma_rules[\"verbs\"]:\n",
    "                if token_text.endswith(old):\n",
    "                    return [token_text[: -len(old)] + new]\n",
    "        \n",
    "        return [token_text]\n",
    "\n",
    "\n",
    "@Language.factory(\"kbd_lemmatizer\")\n",
    "def create_kbd_language_lemmatizer(nlp, name):\n",
    "    return KbdLanguageLemmatizer(nlp.vocab, name=name, mode=\"rule\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T21:39:16.534090Z",
     "start_time": "2024-02-22T21:39:16.525349Z"
    }
   },
   "id": "f34f993e7789aa23",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "784764"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"kbd\")\n",
    "nlp.initialize()\n",
    "\n",
    "len(nlp.vocab.strings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T21:39:17.914883Z",
     "start_time": "2024-02-22T21:39:17.898213Z"
    }
   },
   "id": "a9c6dade9f1f2656",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<__main__.KbdLanguageLemmatizer at 0x103522e10>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Добавляем нашу функцию в пайплайн\n",
    "nlp.add_pipe(\"custom_prefix_suffix\")\n",
    "nlp.add_pipe(\"simple_pos_tagger\")\n",
    "nlp.add_pipe(\"kbd_lemmatizer\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T21:39:19.116740Z",
     "start_time": "2024-02-22T21:39:19.108256Z"
    }
   },
   "id": "e3f54be95afb8a3b",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "къитхыкIыж|къ|итхы|кIыж|къитхыкIыжын|VERB|\n",
      "къиcтхыкIащ|къ|иcтхыкI|ащ|къитхыкIын|VERB|\n",
      "къритхыкIынущ|къ|ритхыкI|ынущ|къитхыкIын|X|\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"къитхыкIыж къиcтхыкIащ къритхыкIынущ\")\n",
    "for token in doc:\n",
    "    print('|'.join([token.text, token._.custom_prefix, token._.custom_stem, token._.custom_suffix, token.lemma_, token.pos_, token.tag_]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T21:39:20.036253Z",
     "start_time": "2024-02-22T21:39:20.029557Z"
    }
   },
   "id": "985b40455943da8a",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'къритхыкIынущ' in nlp.vocab.strings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T21:39:20.797209Z",
     "start_time": "2024-02-22T21:39:20.794241Z"
    }
   },
   "id": "5b5e6466240c19c3",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['къитхыкIыж'].has_vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T21:40:06.397532Z",
     "start_time": "2024-02-22T21:40:06.395749Z"
    }
   },
   "id": "df75c7889d0ade5e",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d768596f7573f095"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
