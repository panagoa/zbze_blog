{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "В этом ноутбуке мы будем анализировать распределение токенов и их комбинаций в словах.\n",
    "\n",
    "Используя токенизатор и N-граммный анализ, мы исследуем повторяющиеся паттерны, что поможет в задачах стемминга и лемматизации."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer_uni = Tokenizer.from_file('../data/processed/tokenizer/words_unigram_5000.tokenizer.json')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T02:01:47.312510Z",
     "start_time": "2024-01-04T02:01:47.311273Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "with open('../data/processed/word_freqs/freq_1000000_oshhamaho.txt') as f:\n",
    "    words = f.read().split('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T02:01:48.485896Z",
     "start_time": "2024-01-04T02:01:48.227748Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "token_dist_dir = '../data/processed/token_distribution'\n",
    "os.makedirs(token_dist_dir, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T02:01:49.298521Z",
     "start_time": "2024-01-04T02:01:49.285671Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def create_token_ng_distribution(words, n=5):\n",
    "    token_ngrams_freq_dist = nltk.FreqDist()\n",
    "    ngrams_tokens = defaultdict(list)\n",
    "\n",
    "    pgbar = tqdm(sorted(words), desc=f'create {n}-gram distribution')\n",
    "    for word in pgbar:\n",
    "        tokens = tokenizer_uni.encode(word).tokens\n",
    "        token_ids = [tokenizer_uni.token_to_id(token) for token in tokens]\n",
    "\n",
    "        ngrams = tuple(nltk.ngrams(tokens, n=n))\n",
    "        token_ngrams_freq_dist.update(ngrams)\n",
    "        for ng in ngrams:\n",
    "            ngrams_tokens[ng].append((tokens, token_ids))\n",
    "\n",
    "    return token_ngrams_freq_dist, ngrams_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T02:01:50.087400Z",
     "start_time": "2024-01-04T02:01:50.085498Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def is_valid_stem(stem, freq, min_freq=10, max_freq=100, min_len=3):\n",
    "    # малая частота стема говорит о том что она возможно слишком специфичная и длинная\n",
    "    if freq < min_freq:\n",
    "        return False\n",
    "\n",
    "    # большая частота говорит о том что она возможно слишком общая и короткая\n",
    "    if freq > max_freq:\n",
    "        return False\n",
    "\n",
    "    # если длина стема слишком мала, кандидат тоже не подходит\n",
    "    if len(stem) < min_len:\n",
    "        return False\n",
    "\n",
    "    # todo можно проверить на вхождение в список префиксов и суффиксов, так точность должна увеличиться\n",
    "    return True\n",
    "\n",
    "\n",
    "def save_by_stem(df_data, n, ng_name, freq):\n",
    "    df = pd.DataFrame(df_data)\n",
    "    # сортируя по количеству токенов в слове, наверху будут слова слова близкие к корню, а внизу сложносоставные слова\n",
    "    df = df.sort_values('word_ng_len', ascending=True)\n",
    "\n",
    "    f_name = f'({freq}){ng_name}'\n",
    "    f_path = f'{token_dist_dir}/{n}/{f_name}.csv'\n",
    "    df.to_csv(f_path, index=False, sep=',', quoting=csv.QUOTE_MINIMAL, header=True)\n",
    "\n",
    "\n",
    "def choose_stem(token_ngrams_freq_dist, ngrams_tokens, n=5, is_save_by_stem=False):\n",
    "    if is_save_by_stem:\n",
    "        os.makedirs(f'{token_dist_dir}/{n}', exist_ok=True)\n",
    "\n",
    "    data = []\n",
    "    freq_dist_sorted = sorted(token_ngrams_freq_dist.items(), key=lambda x: x[1], reverse=True)\n",
    "    pgbar = tqdm(freq_dist_sorted, desc=f'choose {n}-gram stem')\n",
    "    for ng, freq in pgbar:\n",
    "        # фильтруя n-граммы по частоте, получим часть слова которая встречается в составе других слов\n",
    "        # то есть часть слова которую можно использовать для поиска похожих слов (стемминг)\n",
    "\n",
    "        stem = ''.join(ng)\n",
    "        if not is_valid_stem(stem, freq):\n",
    "            continue\n",
    "\n",
    "        df_data = []\n",
    "        for _tokens, _token_ids in ngrams_tokens[ng]:\n",
    "            word = ''.join(_tokens)\n",
    "            df_data.append({\n",
    "                'stem_ng_len': n,\n",
    "                'stem': stem,\n",
    "                'word_ng_len': len(_tokens),\n",
    "                'word': word,\n",
    "                'template': word.replace(stem, '?'*len(stem)),\n",
    "                'tokens': '|'.join(_tokens),\n",
    "            })\n",
    "\n",
    "        if is_save_by_stem:\n",
    "            ng_name = '_'.join(ng)\n",
    "            save_by_stem(df_data, n, ng_name, freq)\n",
    "\n",
    "        data.extend(df_data)\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T02:01:50.857328Z",
     "start_time": "2024-01-04T02:01:50.846569Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "create 1-gram distribution: 100%|██████████| 483338/483338 [00:07<00:00, 68899.23it/s]\n",
      "choose 1-gram stem: 100%|██████████| 2695/2695 [00:00<00:00, 63203.25it/s]\n",
      "create 2-gram distribution: 100%|██████████| 483338/483338 [00:06<00:00, 73371.59it/s]\n",
      "choose 2-gram stem: 100%|██████████| 152176/152176 [00:00<00:00, 335584.37it/s]\n",
      "create 3-gram distribution: 100%|██████████| 483338/483338 [00:06<00:00, 75492.05it/s]\n",
      "choose 3-gram stem: 100%|██████████| 364413/364413 [00:00<00:00, 817025.21it/s]\n",
      "create 4-gram distribution: 100%|██████████| 483338/483338 [00:06<00:00, 76733.62it/s] \n",
      "choose 4-gram stem: 100%|██████████| 389478/389478 [00:00<00:00, 1403925.22it/s]\n",
      "create 5-gram distribution: 100%|██████████| 483338/483338 [00:05<00:00, 94744.77it/s] \n",
      "choose 5-gram stem: 100%|██████████| 308352/308352 [00:00<00:00, 2130043.64it/s]\n",
      "create 6-gram distribution: 100%|██████████| 483338/483338 [00:05<00:00, 95185.60it/s] \n",
      "choose 6-gram stem: 100%|██████████| 191758/191758 [00:00<00:00, 2781917.67it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for n in range(1, 7):\n",
    "    token_fd, ngrams_tokens = create_token_ng_distribution(words, n=n)\n",
    "    data_i = choose_stem(token_fd, ngrams_tokens, n=n, is_save_by_stem=False)\n",
    "    data.extend(data_i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T02:02:32.667774Z",
     "start_time": "2024-01-04T02:01:52.243978Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "stem_df = pd.DataFrame(data)\n",
    "# Если стема полностью повторяет слово, она выбрана вполне удачно.\n",
    "# В некоторых случаях возможно что это даже лемма, ну или поиск леммы сильно упрощается\n",
    "\n",
    "# здесь не обязательно сравнивать строки полностью, достаточно количество токенов\n",
    "stem_df['stem_is_full_word'] = stem_df['stem_ng_len'] == stem_df['word_ng_len']\n",
    "\n",
    "sort_values = ['stem_ng_len', 'stem', 'word_ng_len', 'word']\n",
    "stem_df = stem_df.sort_values(sort_values, ascending=True)\n",
    "stem_df.to_csv(f'{token_dist_dir}/stem_candidates.csv.gz', index=False, sep=',', quoting=csv.QUOTE_MINIMAL, header=True, compression='gzip')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T02:02:49.896976Z",
     "start_time": "2024-01-04T02:02:34.635867Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "        stem_ng_len    stem  word_ng_len          word      template  \\\n448267            2  джэгун            2        джэгун        ??????   \n448266            2  джэгун            3      Сыджэгун      Сы??????   \n448268            2  джэгун            3      джэгунми      ??????ми   \n448269            2  джэгун            3      джэгунри      ??????ри   \n448270            2  джэгун            3      джэгунрэ      ??????рэ   \n448272            2  джэгун            3      дыджэгун      ды??????   \n448273            2  джэгун            3    зэрыджэгун    зэры??????   \n448277            2  джэгун            3       уджэгун       у??????   \n448280            2  джэгун            3  фызэрыджэгун  фызэры??????   \n448271            2  джэгун            4     джэгунрэт     ??????рэт   \n448274            2  джэгун            4     ириджэгун     ири??????   \n448275            2  джэгун            4     къиджэгун     къи??????   \n448278            2  джэгун            4     уджэгунри     у??????ри   \n448276            2  джэгун            5    къэбджэгун    къэб??????   \n448279            2  джэгун            5   уриджэгунри   ури??????ри   \n\n                 tokens  stem_is_full_word  \n448267          джэгу|н               True  \n448266       Сы|джэгу|н              False  \n448268       джэгу|н|ми              False  \n448269       джэгу|н|ри              False  \n448270       джэгу|н|рэ              False  \n448272       ды|джэгу|н              False  \n448273     зэры|джэгу|н              False  \n448277        у|джэгу|н              False  \n448280   фызэры|джэгу|н              False  \n448271     джэгу|н|рэ|т              False  \n448274     и|ри|джэгу|н              False  \n448275     къ|и|джэгу|н              False  \n448278     у|джэгу|н|ри              False  \n448276   къ|э|б|джэгу|н              False  \n448279  у|ри|джэгу|н|ри              False  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stem_ng_len</th>\n      <th>stem</th>\n      <th>word_ng_len</th>\n      <th>word</th>\n      <th>template</th>\n      <th>tokens</th>\n      <th>stem_is_full_word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>448267</th>\n      <td>2</td>\n      <td>джэгун</td>\n      <td>2</td>\n      <td>джэгун</td>\n      <td>??????</td>\n      <td>джэгу|н</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>448266</th>\n      <td>2</td>\n      <td>джэгун</td>\n      <td>3</td>\n      <td>Сыджэгун</td>\n      <td>Сы??????</td>\n      <td>Сы|джэгу|н</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>448268</th>\n      <td>2</td>\n      <td>джэгун</td>\n      <td>3</td>\n      <td>джэгунми</td>\n      <td>??????ми</td>\n      <td>джэгу|н|ми</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>448269</th>\n      <td>2</td>\n      <td>джэгун</td>\n      <td>3</td>\n      <td>джэгунри</td>\n      <td>??????ри</td>\n      <td>джэгу|н|ри</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>448270</th>\n      <td>2</td>\n      <td>джэгун</td>\n      <td>3</td>\n      <td>джэгунрэ</td>\n      <td>??????рэ</td>\n      <td>джэгу|н|рэ</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>448272</th>\n      <td>2</td>\n      <td>джэгун</td>\n      <td>3</td>\n      <td>дыджэгун</td>\n      <td>ды??????</td>\n      <td>ды|джэгу|н</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>448273</th>\n      <td>2</td>\n      <td>джэгун</td>\n      <td>3</td>\n      <td>зэрыджэгун</td>\n      <td>зэры??????</td>\n      <td>зэры|джэгу|н</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>448277</th>\n      <td>2</td>\n      <td>джэгун</td>\n      <td>3</td>\n      <td>уджэгун</td>\n      <td>у??????</td>\n      <td>у|джэгу|н</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>448280</th>\n      <td>2</td>\n      <td>джэгун</td>\n      <td>3</td>\n      <td>фызэрыджэгун</td>\n      <td>фызэры??????</td>\n      <td>фызэры|джэгу|н</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>448271</th>\n      <td>2</td>\n      <td>джэгун</td>\n      <td>4</td>\n      <td>джэгунрэт</td>\n      <td>??????рэт</td>\n      <td>джэгу|н|рэ|т</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>448274</th>\n      <td>2</td>\n      <td>джэгун</td>\n      <td>4</td>\n      <td>ириджэгун</td>\n      <td>ири??????</td>\n      <td>и|ри|джэгу|н</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>448275</th>\n      <td>2</td>\n      <td>джэгун</td>\n      <td>4</td>\n      <td>къиджэгун</td>\n      <td>къи??????</td>\n      <td>къ|и|джэгу|н</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>448278</th>\n      <td>2</td>\n      <td>джэгун</td>\n      <td>4</td>\n      <td>уджэгунри</td>\n      <td>у??????ри</td>\n      <td>у|джэгу|н|ри</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>448276</th>\n      <td>2</td>\n      <td>джэгун</td>\n      <td>5</td>\n      <td>къэбджэгун</td>\n      <td>къэб??????</td>\n      <td>къ|э|б|джэгу|н</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>448279</th>\n      <td>2</td>\n      <td>джэгун</td>\n      <td>5</td>\n      <td>уриджэгунри</td>\n      <td>ури??????ри</td>\n      <td>у|ри|джэгу|н|ри</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_df[stem_df['stem'] == 'джэгун'].head(n=30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T02:02:51.641215Z",
     "start_time": "2024-01-04T02:02:51.639110Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "replace_many_q = lambda x: re.sub(r'\\?{1,}', '*', x)\n",
    "\n",
    "templates_cnt = Counter([replace_many_q(tmpl) for tmpl in stem_df['template'].values])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T02:02:53.347955Z",
     "start_time": "2024-01-04T02:02:52.593615Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "templates_df = pd.DataFrame(templates_cnt.most_common(), columns=['template', 'freq'])\n",
    "templates_df = templates_df[templates_df['freq'] > 10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T02:02:54.247658Z",
     "start_time": "2024-01-04T02:02:54.144809Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "templates_df.to_csv(f'{token_dist_dir}/stem_templates.csv', index=False, sep=',', quoting=csv.QUOTE_MINIMAL, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T02:02:54.257773Z",
     "start_time": "2024-01-04T02:02:54.248924Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
