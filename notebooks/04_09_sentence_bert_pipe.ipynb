{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.9050441099916186"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def calculate_similarity_score(sent1, sent2, embedding1, embedding2, alpha=0.7, beta=0.2, gamma=0.1):\n",
    "    cosine_sim = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "    \n",
    "    length_diff = abs(len(sent1) - len(sent2)) / (len(sent1) + len(sent2))\n",
    "    \n",
    "    vectorizer = CountVectorizer().fit([sent1, sent2])\n",
    "    vectors = vectorizer.transform([sent1, sent2])\n",
    "    word_overlap = vectors[0].dot(vectors[1].T).toarray()[0][0] / (vectors[0].sum() + vectors[1].sum() - vectors[0].dot(vectors[1].T).toarray()[0][0])\n",
    "    \n",
    "    similarity_score = alpha * cosine_sim + beta * (1 - length_diff) + gamma * word_overlap\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "model = SentenceTransformer('./sbert_from_mlm_bert_80')\n",
    "\n",
    "sent1 = '– Тхьэ соIуэ, си пыIэкур къивудакIэ!'\n",
    "sent2 = '– Тхьэ соIуэ, ткIуэпс сIумыхуакIэ!'\n",
    "\n",
    "calculate_similarity_score(\n",
    "    sent1=sent1,\n",
    "    sent2=sent2,\n",
    "    embedding1=model.encode(sent1),\n",
    "    embedding2=model.encode(sent2)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T23:51:42.268747Z",
     "start_time": "2024-04-15T23:51:39.174740Z"
    }
   },
   "id": "a4820165240a4a6e",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sentence_transformers import SentencesDataset, InputExample, losses\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T23:51:43.604945Z",
     "start_time": "2024-04-15T23:51:43.587925Z"
    }
   },
   "id": "dd741d41225be5fd",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_data_for_training(sent_pairs_df, model):\n",
    "    train_examples = []\n",
    "\n",
    "    for sent_1, sent_2 in tqdm(sent_pairs_df[['sent1', 'sent2']].values):\n",
    "        score = calculate_similarity_score(\n",
    "            sent1=sent1,\n",
    "            sent2=sent2,\n",
    "            embedding1=model.encode(sent1),\n",
    "            embedding2=model.encode(sent2)\n",
    "        )\n",
    "\n",
    "        train_examples.append(\n",
    "            InputExample(texts=[sent_1, sent_2], label=round(float(score), 2))\n",
    "        )\n",
    "    return train_examples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T23:51:44.564248Z",
     "start_time": "2024-04-15T23:51:44.284488Z"
    }
   },
   "id": "3645987ea192531c",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run_training(from_model_path: str, to_model_path: str, train_examples):\n",
    "    model = SentenceTransformer(from_model_path)\n",
    "\n",
    "    # Создание и загрузка датасета\n",
    "    train_dataset = SentencesDataset(examples=train_examples, model=model)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Настройка процесса обучения\n",
    "    train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "    # Обучение модели\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=1\n",
    "    )\n",
    "    model.save(to_model_path)\n",
    "\n",
    "    return to_model_path"
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-15T23:52:01.951590Z",
     "start_time": "2024-04-15T23:52:01.940430Z"
    }
   },
   "id": "initial_id",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "def get_sents(text_path: str, len_min: int, len_max: int):\n",
    "    with open(text_path, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    sents = sorted(set([\n",
    "        sent.replace('\\n', ' ')\n",
    "        for sent in nltk.sent_tokenize(text)\n",
    "        if len_min < len(sent) < len_max\n",
    "    ]))\n",
    "    return sents\n",
    "\n",
    "\n",
    "def get_clusters(vectors, n_clusters=20):\n",
    "    agg_clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    labels = agg_clustering.fit_predict(vectors)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_sents_by_clusters(words, labels):\n",
    "    sents_by_clusters = defaultdict(list)\n",
    "    for i, label in enumerate(labels):\n",
    "        sents_by_clusters[label].append(words[i])\n",
    "\n",
    "    return sents_by_clusters\n",
    "\n",
    "\n",
    "def clusterize_sents(sents, model, version, butch_size=10000, cluster_num=1000):\n",
    "    cluster_factor = butch_size / cluster_num\n",
    "\n",
    "    for seed in range(111, 115):\n",
    "        export_path = f'../data/processed/sent_clusters_{version}/seed_{seed}/{cluster_factor}_{butch_size}_{cluster_num}'\n",
    "        os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "        random.shuffle(sents)\n",
    "\n",
    "        for offset in tqdm(range(0, len(sents), butch_size)):\n",
    "            butch_sents = sents[offset:offset + butch_size]\n",
    "            if len(butch_sents) < cluster_num:\n",
    "                break\n",
    "\n",
    "            word_vectors = [\n",
    "                model.encode(sent)\n",
    "                for sent in butch_sents\n",
    "            ]\n",
    "            labels = get_clusters(word_vectors, n_clusters=cluster_num)\n",
    "            sents_by_clusters = get_sents_by_clusters(butch_sents, labels)\n",
    "\n",
    "            for cluster_label, cluster_sents in sents_by_clusters.items():\n",
    "                with open(f'{export_path}/cluster_{offset}_{offset + butch_size}_{cluster_label}.txt', 'w') as f:\n",
    "                    f.write('\\n'.join(cluster_sents))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T23:52:02.985668Z",
     "start_time": "2024-04-15T23:52:02.853156Z"
    }
   },
   "id": "f421aed747181a44",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def split_sent_by_cluster(sentences, model):\n",
    "    if len(sentences) < 8:\n",
    "        return {0: sentences}\n",
    "\n",
    "    embeddings = model.encode(sentences)\n",
    "\n",
    "    clustering_model = KMeans(n_clusters=4, n_init=20, max_iter=1000)\n",
    "    clustering_model.fit(embeddings)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "\n",
    "    clusters = defaultdict(list)\n",
    "    for sentence, cluster_id in zip(sentences, cluster_assignment):\n",
    "        clusters[cluster_id].append(sentence)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def select_sent_from_clusters(cluster_version, model):\n",
    "    clusters_path = f'../data/processed/sent_clusters_{cluster_version}/'\n",
    "\n",
    "    sent_pairs = []\n",
    "    for seed_dir in os.listdir(clusters_path):\n",
    "        seed_path = f'{clusters_path}/{seed_dir}'\n",
    "        if not os.path.isdir(seed_path):\n",
    "            continue\n",
    "\n",
    "        for cluster_dir in os.listdir(seed_path):\n",
    "            clusters_path = f'{clusters_path}/{seed_dir}/{cluster_dir}'\n",
    "            if not os.path.isdir(clusters_path):\n",
    "                continue\n",
    "\n",
    "            anti_pairs = []\n",
    "            for cluster_file in tqdm(os.listdir(clusters_path)):\n",
    "                if not cluster_file.endswith('.txt'):\n",
    "                    continue\n",
    "\n",
    "                cluster_file_path = f'{clusters_path}/{cluster_file}'\n",
    "                with open(cluster_file_path, 'r') as f:\n",
    "                    sents = f.read().split('\\n')\n",
    "\n",
    "                if len(sents) < 2:\n",
    "                    continue\n",
    "\n",
    "                for group, sentences in split_sent_by_cluster(\n",
    "                        sents, model\n",
    "                ).items():\n",
    "                    for s1 in sentences:\n",
    "                        for s2 in sentences:\n",
    "                            if s1 == s2:\n",
    "                                continue\n",
    "                            sent_pairs.append(('pos', s1, s2)\n",
    "                                              )\n",
    "\n",
    "                if len(sent_pairs) < 1000:\n",
    "                    continue\n",
    "\n",
    "                for sent in sents:\n",
    "                    for _ in range(len(sents)):\n",
    "                        anti_pairs.append(('neg', sent, random.choice(sent_pairs)[2]))\n",
    "\n",
    "            sent_pairs.extend(anti_pairs)\n",
    "\n",
    "    sent_pairs_df = pd.DataFrame(sent_pairs, columns=['type', 'sent1', 'sent2'])\n",
    "    sent_pairs_df.drop_duplicates(subset=['sent1', 'sent2'], inplace=True)\n",
    "    sent_pairs_df['final_score'] = sent_pairs_df['type'].apply(lambda x: 1 if x == 'pos' else 0)\n",
    "    \n",
    "    slice_df = pd.DataFrame(\n",
    "        sent_pairs_df.groupby('sent1').apply(lambda x: x.sample(n=min(len(x), 5), random_state=1)).reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    slice_df.to_csv(f'../data/processed/sent_pairs_{cluster_version}.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T23:52:03.726114Z",
     "start_time": "2024-04-15T23:52:03.718517Z"
    }
   },
   "id": "1db2e5fc41609629",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2553/139843 [00:34<30:35, 74.81it/s]"
     ]
    }
   ],
   "source": [
    "for iteration in range(80, 85):\n",
    "    from_model_path = f'./sbert_from_mlm_bert_{iteration}'\n",
    "    to_model_path = f'./sbert_from_mlm_bert_{iteration + 1}'\n",
    "\n",
    "    model_from = SentenceTransformer(from_model_path)\n",
    "\n",
    "    sentence = get_sents('../data/processed/oshhamaho.txt', 30, 40)\n",
    "    clusterize_sents(sents=sentence, model=model_from, version=iteration, butch_size=5000, cluster_num=500)\n",
    "    select_sent_from_clusters(cluster_version=iteration, model=model_from)\n",
    "\n",
    "    sent_pairs_df = pd.read_csv(f'../data/processed/sent_pairs_{iteration}.csv')\n",
    "    train_examples = prepare_data_for_training(sent_pairs_df, model_from)\n",
    "    run_training(from_model_path, to_model_path, train_examples)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-15T23:52:19.464841Z"
    }
   },
   "id": "9c46d67a5ea61d16",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0ec01b8e6b8005b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
