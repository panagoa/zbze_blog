{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd741d41225be5fd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T01:12:18.519830Z",
     "start_time": "2024-03-18T01:12:15.764541Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentencesDataset, InputExample, losses\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3645987ea192531c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T01:12:20.665916Z",
     "start_time": "2024-03-18T01:12:20.171283Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_data_for_training(sent_pairs_df: pd.DataFrame = None):\n",
    "    train_examples = []\n",
    "\n",
    "    for sent_1, sent_2, final_score in sent_pairs_df[['sent1', 'sent2', 'final_score']].values:\n",
    "        train_examples.append(\n",
    "            InputExample(texts=[sent_1, sent_2], label=round(float(final_score), 2))\n",
    "        )\n",
    "    return train_examples"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run_training(from_model_path: str, to_model_path: str, train_examples):\n",
    "    model = SentenceTransformer(from_model_path)\n",
    "\n",
    "    # Создание и загрузка датасета\n",
    "    train_dataset = SentencesDataset(examples=train_examples, model=model)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Настройка процесса обучения\n",
    "    train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "    # Обучение модели\n",
    "    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1)\n",
    "    model.save(to_model_path)\n",
    "\n",
    "    return to_model_path"
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-03-18T01:12:21.652292Z",
     "start_time": "2024-03-18T01:12:21.643564Z"
    }
   },
   "id": "initial_id",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "def get_sents(text_path: str, len_min: int, len_max: int):\n",
    "    with open(text_path, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    sents = sorted(set([\n",
    "        sent.replace('\\n', ' ')\n",
    "        for sent in nltk.sent_tokenize(text)\n",
    "        if len_min < len(sent) < len_max\n",
    "    ]))\n",
    "    return sents\n",
    "\n",
    "\n",
    "def get_clusters(vectors, n_clusters=20):\n",
    "    agg_clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    labels = agg_clustering.fit_predict(vectors)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_sents_by_clusters(words, labels):\n",
    "    sents_by_clusters = defaultdict(list)\n",
    "    for i, label in enumerate(labels):\n",
    "        sents_by_clusters[label].append(words[i])\n",
    "\n",
    "    return sents_by_clusters\n",
    "\n",
    "\n",
    "def clusterize_sents(sents, model, version, butch_size=10000, cluster_num=1000):\n",
    "    cluster_factor = butch_size / cluster_num\n",
    "\n",
    "    for seed in range(111, 115):\n",
    "        export_path = f'../data/processed/alignment_sent_clusters_{version}/seed_{seed}/{cluster_factor}_{butch_size}_{cluster_num}'\n",
    "        os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "        random.shuffle(sents)\n",
    "\n",
    "        for offset in tqdm(range(0, len(sents), butch_size)):\n",
    "            butch_sents = sents[offset:offset + butch_size]\n",
    "            if len(butch_sents) < cluster_num:\n",
    "                break\n",
    "\n",
    "            word_vectors = [\n",
    "                model.encode(sent)\n",
    "                for sent in butch_sents\n",
    "            ]\n",
    "            labels = get_clusters(word_vectors, n_clusters=cluster_num)\n",
    "            sents_by_clusters = get_sents_by_clusters(butch_sents, labels)\n",
    "\n",
    "            for cluster_label, cluster_sents in sents_by_clusters.items():\n",
    "                with open(f'{export_path}/cluster_{offset}_{offset + butch_size}_{cluster_label}.txt', 'w') as f:\n",
    "                    f.write('\\n'.join(cluster_sents))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T01:12:22.259262Z",
     "start_time": "2024-03-18T01:12:22.167788Z"
    }
   },
   "id": "f421aed747181a44",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def split_sent_by_cluster(sentences, model):\n",
    "    if len(sentences) < 8:\n",
    "        return {0: sentences}\n",
    "\n",
    "    embeddings = model.encode(sentences)\n",
    "\n",
    "    clustering_model = KMeans(n_clusters=4, n_init=20, max_iter=1000)\n",
    "    clustering_model.fit(embeddings)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "\n",
    "    clusters = defaultdict(list)\n",
    "    for sentence, cluster_id in zip(sentences, cluster_assignment):\n",
    "        clusters[cluster_id].append(sentence)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def select_sent_from_clusters(cluster_version, model):\n",
    "    clusters_path = f'../data/processed/alignment_sent_clusters_{cluster_version}/'\n",
    "\n",
    "    sent_pairs = []\n",
    "    for seed_dir in os.listdir(clusters_path):\n",
    "        seed_path = f'{clusters_path}/{seed_dir}'\n",
    "        if not os.path.isdir(seed_path):\n",
    "            continue\n",
    "\n",
    "        for cluster_dir in os.listdir(seed_path):\n",
    "            clusters_path = f'{clusters_path}/{seed_dir}/{cluster_dir}'\n",
    "            if not os.path.isdir(clusters_path):\n",
    "                continue\n",
    "\n",
    "            anti_pairs = []\n",
    "            for cluster_file in tqdm(os.listdir(clusters_path)):\n",
    "                if not cluster_file.endswith('.txt'):\n",
    "                    continue\n",
    "\n",
    "                cluster_file_path = f'{clusters_path}/{cluster_file}'\n",
    "                with open(cluster_file_path, 'r') as f:\n",
    "                    sents = f.read().split('\\n')\n",
    "\n",
    "                if len(sents) < 2:\n",
    "                    continue\n",
    "\n",
    "                for group, sentences in split_sent_by_cluster(\n",
    "                        sents, model\n",
    "                ).items():\n",
    "                    for s1 in sentences:\n",
    "                        for s2 in sentences:\n",
    "                            if s1 == s2:\n",
    "                                continue\n",
    "                            sent_pairs.append(('pos', s1, s2)\n",
    "                                              )\n",
    "\n",
    "                if len(sent_pairs) < 1000:\n",
    "                    continue\n",
    "\n",
    "                for sent in sents:\n",
    "                    for _ in range(len(sents)):\n",
    "                        anti_pairs.append(('neg', sent, random.choice(sent_pairs)[2]))\n",
    "\n",
    "            sent_pairs.extend(anti_pairs)\n",
    "\n",
    "    sent_pairs_df = pd.DataFrame(sent_pairs, columns=['type', 'sent1', 'sent2'])\n",
    "    sent_pairs_df.drop_duplicates(subset=['sent1', 'sent2'], inplace=True)\n",
    "    sent_pairs_df['final_score'] = sent_pairs_df['type'].apply(lambda x: 1 if x == 'pos' else 0)\n",
    "    \n",
    "    slice_df = pd.DataFrame(\n",
    "        sent_pairs_df.groupby('sent1').apply(lambda x: x.sample(n=min(len(x), 5), random_state=1)).reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    slice_df.to_csv(f'../data/processed/sent_pairs_{cluster_version}.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T01:12:22.784100Z",
     "start_time": "2024-03-18T01:12:22.777219Z"
    }
   },
   "id": "1db2e5fc41609629",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [03:32<03:34, 71.58s/it]"
     ]
    }
   ],
   "source": [
    "for iteration in range(21, 30):\n",
    "    from_model_path = f'./sbert_from_mlm_bert_{iteration}'\n",
    "    to_model_path = f'./sbert_from_mlm_bert_{iteration + 1}'\n",
    "\n",
    "    model_from = SentenceTransformer(from_model_path)\n",
    "\n",
    "    sentence = get_sents('../data/processed/oshhamaho.txt', 20, 40)\n",
    "    clusterize_sents(sents=sentence, model=model_from, version=iteration, butch_size=10000, cluster_num=1000)\n",
    "    select_sent_from_clusters(cluster_version=iteration, model=model_from)\n",
    "\n",
    "    sent_pairs_df = pd.read_csv(f'../data/processed/sent_pairs_{iteration}.csv')\n",
    "    train_examples = prepare_data_for_training(sent_pairs_df)\n",
    "    run_training(from_model_path, to_model_path, train_examples)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-18T01:12:25.437591Z"
    }
   },
   "id": "9c46d67a5ea61d16",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c513c27caac35b3c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
