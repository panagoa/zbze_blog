{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-25T23:47:21.732247Z",
     "start_time": "2023-12-25T23:47:21.728585Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "\n",
    "tokenizer = Tokenizer(Unigram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "custom_tokens = [\n",
    "    'гу',\n",
    "    'гъ',\n",
    "    'гъу',\n",
    "    'дж',\n",
    "    'дз',\n",
    "    'жь',\n",
    "    'ку',\n",
    "    'кӀ',\n",
    "    'кIу',\n",
    "    'къ',\n",
    "    'къу',\n",
    "    'кхъ',\n",
    "    'кхъу',\n",
    "    'лъ',\n",
    "    'лI',\n",
    "    'пI',\n",
    "    'тI',\n",
    "    'фI',\n",
    "    'ху',\n",
    "    'хь',\n",
    "    'хъ',\n",
    "    'хъу',\n",
    "    'цI',\n",
    "    'щI',\n",
    "    'Ӏу',\n",
    "    'хуэ',\n",
    "]\n",
    "\n",
    "tokenizer.add_tokens(custom_tokens)\n",
    "\n",
    "vocab_size = 30000\n",
    "\n",
    "trainer = UnigramTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    show_progress=True,\n",
    "    unk_token='UNK'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-12-26T01:06:42.770684Z",
     "start_time": "2023-12-26T01:06:42.769090Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для unigram токенизатора возьмем файл не из сплошного текста, а из уже выделенных слов.\n",
    "Таким образом мы получим представление об словообразовании, без влияния популярности слов - так как слова будут представлены в единственном экземпляре.   "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "file_dir = '../data/processed/word_freqs'\n",
    "file_name = 'freq_1000000_oshhamaho.txt'\n",
    "file_path = os.path.join(file_dir, file_name)\n",
    "\n",
    "tokenizer.train([file_path], trainer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T01:07:26.304853Z",
     "start_time": "2023-12-26T01:06:44.283368Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "tokenizer_dir = '../data/processed/tokenizer'\n",
    "tokenizer_file_name = f'words_unigram_{vocab_size}.tokenizer.json'\n",
    "tokenizer_file_path = os.path.join(tokenizer_dir, tokenizer_file_name)\n",
    "tokenizer.save(tokenizer_file_path, pretty=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-12-26T01:07:26.313679Z",
     "start_time": "2023-12-26T01:07:26.312528Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ц', 'Ӏ', 'ы', 'ху', 'м', ' ', 'и', ' ', 'гу', 'м', ' ', 'у', 'ды', 'хь', 'эн', ' ', 'пап', 'щ', 'Ӏ', 'э', ',', ' ', 'абы', ' ', 'и', ' ', 'б', 'гъ', 'эр', ' ', 'зэ', 'гу', 'э', 'б', 'гъ', 'эжын', ' ', 'ху', 'ей', 'къ', 'ым', '.']\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('ЦӀыхум и гум удыхьэн папщӀэ, абы и бгъэр зэгуэбгъэжын хуейкъым.')\n",
    "print(encoded.tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-12-26T01:07:26.318868Z",
     "start_time": "2023-12-26T01:07:26.314816Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['дж', 'э', 'гу', 'а', 'кIу', 'э']\n",
      "['що', 'дж', 'э', 'гу']\n",
      "['щы', 'дж', 'э', 'гу', 'ащ']\n",
      "['дж', 'э', 'гу', 'пI', 'э']\n",
      "['и', 'гъ', 'э', 'дж', 'э', 'гу', 'у']\n",
      "['дж', 'э', 'гу', 'ш', 'хуэ']\n",
      "['дыщы', 'дж', 'э', 'гу', 'ащ']\n"
     ]
    }
   ],
   "source": [
    "test_words = [\n",
    "    'джэгуакIуэ',\n",
    "    'щоджэгу',\n",
    "    'щыджэгуащ',\n",
    "    'джэгупIэ',\n",
    "    'игъэджэгуу',\n",
    "    'джэгушхуэ',\n",
    "    'дыщыджэгуащ',\n",
    "]\n",
    "\n",
    "for word in test_words:\n",
    "    print(tokenizer.encode(word).tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T01:15:32.307841Z",
     "start_time": "2023-12-26T01:15:32.299240Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Результат unigram токенизатора получился вполне близким к морфологическим правилам языка.\n",
    "\n",
    "Видим что хорошо разбивает на префиксы, аффиксы и суффиксы "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
