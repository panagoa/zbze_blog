{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd741d41225be5fd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T01:50:54.575175Z",
     "start_time": "2024-03-22T01:50:54.556224Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentencesDataset, InputExample, losses\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3645987ea192531c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T01:50:55.185626Z",
     "start_time": "2024-03-22T01:50:55.173500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_data_for_training(sent_pairs_df: pd.DataFrame = None):\n",
    "    train_examples = []\n",
    "\n",
    "    for sent_1, sent_2 in sent_pairs_df[['rus', 'kbd']].values:\n",
    "        train_examples.append(\n",
    "            InputExample(texts=[sent_1, sent_2], label=round(float(1.0), 2))\n",
    "        )\n",
    "    return train_examples"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run_training(from_model_path: str, to_model_path: str, train_examples):\n",
    "    model = SentenceTransformer(from_model_path)\n",
    "\n",
    "    # Создание и загрузка датасета\n",
    "    train_dataset = SentencesDataset(examples=train_examples, model=model)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Настройка процесса обучения\n",
    "    train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "    # Обучение модели\n",
    "    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1)\n",
    "    model.save(to_model_path)\n",
    "\n",
    "    return to_model_path"
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-03-22T01:51:36.209942Z",
     "start_time": "2024-03-22T01:51:36.196635Z"
    }
   },
   "id": "initial_id",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "def get_sents(text_path: str, len_min: int, len_max: int):\n",
    "    with open(text_path, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    sents = sorted(set([\n",
    "        sent.replace('\\n', ' ')\n",
    "        for sent in nltk.sent_tokenize(text)\n",
    "        if len_min < len(sent) < len_max\n",
    "    ]))\n",
    "    return sents\n",
    "\n",
    "\n",
    "def get_clusters(vectors, n_clusters=20):\n",
    "    agg_clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    labels = agg_clustering.fit_predict(vectors)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_sents_by_clusters(words, labels):\n",
    "    sents_by_clusters = defaultdict(list)\n",
    "    for i, label in enumerate(labels):\n",
    "        sents_by_clusters[label].append(words[i])\n",
    "\n",
    "    return sents_by_clusters\n",
    "\n",
    "\n",
    "def clusterize_sents(sents, model, version, butch_size=10000, cluster_num=1000):\n",
    "    cluster_factor = butch_size / cluster_num\n",
    "\n",
    "    for seed in range(111, 115):\n",
    "        export_path = f'../data/processed/sent_clusters_{version}/seed_{seed}/{cluster_factor}_{butch_size}_{cluster_num}'\n",
    "        os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "        random.shuffle(sents)\n",
    "\n",
    "        for offset in tqdm(range(0, len(sents), butch_size)):\n",
    "            butch_sents = sents[offset:offset + butch_size]\n",
    "            if len(butch_sents) < cluster_num:\n",
    "                break\n",
    "\n",
    "            word_vectors = [\n",
    "                model.encode(sent)\n",
    "                for sent in butch_sents\n",
    "            ]\n",
    "            labels = get_clusters(word_vectors, n_clusters=cluster_num)\n",
    "            sents_by_clusters = get_sents_by_clusters(butch_sents, labels)\n",
    "\n",
    "            for cluster_label, cluster_sents in sents_by_clusters.items():\n",
    "                with open(f'{export_path}/cluster_{offset}_{offset + butch_size}_{cluster_label}.txt', 'w') as f:\n",
    "                    f.write('\\n'.join(cluster_sents))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T01:51:36.681429Z",
     "start_time": "2024-03-22T01:51:36.678228Z"
    }
   },
   "id": "f421aed747181a44",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad3a29bf48404c239108e87845933385"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/21567 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d49de9a226848e6adf1768392707a4b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'./sbert_from_mlm_bert_45_aligned'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_model_path = f'./sbert_from_mlm_bert_45_aligned'\n",
    "to_model_path = f'./sbert_from_mlm_bert_45_aligned'\n",
    "\n",
    "model_from = SentenceTransformer(from_model_path)\n",
    "\n",
    "sent_pairs_df = pd.read_csv(f'../data/processed/word_freqs/freq_1000000_oshhamaho_translated.csv')\n",
    "sent_pairs_df.dropna(inplace=True)\n",
    "\n",
    "train_examples = prepare_data_for_training(sent_pairs_df)\n",
    "run_training(from_model_path, to_model_path, train_examples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T02:25:50.763536Z",
     "start_time": "2024-03-22T01:51:37.284344Z"
    }
   },
   "id": "9c46d67a5ea61d16",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e781f307ead91932"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
