{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-01T21:48:27.443539Z",
     "start_time": "2024-01-01T21:48:27.373959Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pygraphviz as pgv\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer_uni = Tokenizer.from_file('../data/processed/tokenizer/words_unigram_5000.tokenizer.json')\n",
    "with open('../data/processed/word_freqs/freq_1000000_oshhamaho.txt') as f:\n",
    "    words = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "FSM_FILE_DIR = \"../data/processed/word_fsm/\"\n",
    "os.makedirs(FSM_FILE_DIR, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T21:48:27.774036Z",
     "start_time": "2024-01-01T21:48:27.764273Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def customize_node(token, token_index, root_index, custom_tokens=None):\n",
    "    \"\"\"\n",
    "    Кастомизация узла по признакам токена\n",
    "    :param token: \n",
    "    :param token_index: \n",
    "    :param root_index: \n",
    "    :param custom_tokens: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    custom_tokens = custom_tokens or []\n",
    "\n",
    "    node_kw = {\n",
    "        'color': 'black',\n",
    "        'shape': 'circle'\n",
    "    }\n",
    "    if token_index == root_index:\n",
    "        node = token\n",
    "        node_kw['color'] = 'dodgerblue'\n",
    "    elif token_index < root_index:\n",
    "        node = f'{token} ^('  # отмечаем что токен находится левее корня\n",
    "        node_kw['color'] = 'webgreen'\n",
    "    else:\n",
    "        node = f')^ {token}'  # отмечаем что токен находится правее корня\n",
    "        node_kw['color'] = 'firebrick'\n",
    "\n",
    "    if token in custom_tokens:  # помечаем интересующие нас токены прямоугольником\n",
    "        node_kw['shape'] = \"rect\"\n",
    "    \n",
    "    return node, node_kw\n",
    "\n",
    "\n",
    "def add_word_tokens_to_graph(graph, root, tokens):\n",
    "    \"\"\"\n",
    "    Добавление токенов слова в граф\n",
    "    :param graph: \n",
    "    :param root: \n",
    "    :param tokens: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    state = \"^\"\n",
    "    root_index = tokens.index(root)\n",
    "    for index, token in enumerate(tokens):\n",
    "        node, node_kw = customize_node(token, token_index=index, root_index=root_index)\n",
    "        graph.add_node(node, **node_kw)\n",
    "        graph.add_edge(state, node, label=token, color=node_kw.get('color'))\n",
    "\n",
    "        state = node\n",
    "\n",
    "    graph.get_node(state).attr[\"shape\"] = \"doublecircle\"\n",
    "\n",
    "\n",
    "def build_words_fsm_by_tokens(words, root, tokenizer):\n",
    "    \"\"\"\n",
    "    Создание автомата по токенам слов\n",
    "    :param words: \n",
    "    :param root: \n",
    "    :param tokenizer: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    graph = pgv.AGraph(directed=True, rankdir=\"LR\")\n",
    "    graph.add_node(\"start\", shape=\"circle\")\n",
    "\n",
    "    for word in words:\n",
    "        tokens = tokenizer.encode(word).tokens\n",
    "        if root not in tokens:\n",
    "            continue\n",
    "\n",
    "        add_word_tokens_to_graph(graph=graph, root=root, tokens=tokens)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def save_fsm(fsm, root, filedir=FSM_FILE_DIR):\n",
    "    filename = root\n",
    "    filepath = os.path.join(filedir, filename)\n",
    "\n",
    "    fsm.layout(prog=\"dot\")\n",
    "    # fsm.draw(f\"{filepath}.png\")  # для больших графов png получаются гигантскими\n",
    "    fsm.draw(f\"{filepath}.svg\", 'svg:cairo')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T21:48:28.215799Z",
     "start_time": "2024-01-01T21:48:28.210821Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "word_root = 'гъэджэгу'\n",
    "root_words = [word for word in words if word_root in word]\n",
    "\n",
    "tokenizer_uni.add_tokens([word_root])\n",
    "\n",
    "fsm = build_words_fsm_by_tokens(words=root_words, root=word_root, tokenizer=tokenizer_uni)\n",
    "save_fsm(fsm=fsm, root=word_root)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T21:48:29.185779Z",
     "start_time": "2024-01-01T21:48:28.821643Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Визуализация морфем в виде графа улучшит понимание структуры слов и морфологических связей"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
