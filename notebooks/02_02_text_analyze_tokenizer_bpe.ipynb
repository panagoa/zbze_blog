{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-25T22:57:03.667959Z",
     "start_time": "2023-12-25T22:57:03.652719Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "tokenizer = Tokenizer(BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "custom_tokens = [\n",
    "    'гу',\n",
    "    'гъ',\n",
    "    'гъу',\n",
    "    'дж',\n",
    "    'дз',\n",
    "    'жь',\n",
    "    'ку',\n",
    "    'кӀ',\n",
    "    'кIу',\n",
    "    'къ',\n",
    "    'къу',\n",
    "    'кхъ',\n",
    "    'кхъу',\n",
    "    'лъ',\n",
    "    'лI',\n",
    "    'пI',\n",
    "    'тI',\n",
    "    'фI',\n",
    "    'ху',\n",
    "    'хь',\n",
    "    'хъ',\n",
    "    'хъу',\n",
    "    'цI',\n",
    "    'щI',\n",
    "    'Ӏу',\n",
    "    'хуэ',\n",
    "]\n",
    "\n",
    "tokenizer.add_tokens(custom_tokens)\n",
    "\n",
    "vocab_size = 100000\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "file_dir = '../data/processed/'\n",
    "file_name = 'oshhamaho.txt'\n",
    "file_path = os.path.join(file_dir, file_name)\n",
    "\n",
    "tokenizer.train([file_path], trainer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-12-25T23:21:36.642131Z",
     "start_time": "2023-12-25T23:20:13.272329Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "tokenizer_dir = '../data/processed/tokenizer'\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "tokenizer_file_name = f'bpe_{vocab_size}.tokenizer.json'\n",
    "tokenizer_file_path = os.path.join(tokenizer_dir, tokenizer_file_name)\n",
    "tokenizer.save(tokenizer_file_path, pretty=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-12-25T23:21:36.685755Z",
     "start_time": "2023-12-25T23:21:36.643136Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ц', 'ы', 'ху', 'м и ', 'гу', 'м ', 'уды', 'хь', 'э', 'н пап', 'щэ', ', ', 'абы и ', 'б', 'гъ', 'эр ', 'зэ', 'гу', 'э', 'б', 'гъ', 'э', 'жын ', 'ху', 'ей', 'къ', 'ы', 'м', '.']\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('ЦӀыхум и гум удыхьэн папщӀэ, абы и бгъэр зэгуэбгъэжын хуейкъым.')\n",
    "print(encoded.tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-12-25T23:55:51.214764Z",
     "start_time": "2023-12-25T23:55:51.199015Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
