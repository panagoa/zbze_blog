# Кабардинский язык в Цифровом Мире: Этап Сбора Данных 

### **Введение**

**В эпоху цифровизации, когда мир становится все более глобализированным, многие малоресурсные языки, включая кабардинский, сталкиваются с угрозой исчезновения. Основная проблема этих языков – не уменьшение числа носителей, а недостаток цифровой поддержки и ограниченное присутствие в интернете, что в будущем может оказать серьезное влияние на их выживание. Кабардинский язык, богатый своей историей и культурой, заслуживает особого внимания в свете сохранения культурного наследия.**

**Цель этой статьи - исследовать процесс сбора текстов на кабардинском языке из цифровых источников, таких как новостные сайты. Эти источники представляют собой ценный ресурс для анализа современного использования языка. В дальнейшем планируется изучение других методов сбора данных, включая материалы из социальных сетей и тексты книг.**

**Выбор кабардинского языка для данного исследования обусловлен его значением как моего родного языка и стремлением сохранить его уникальное культурное наследие. Эта статья представляет собой шаг к цифровому возрождению кабардинского языка и привлечению внимания специалистов в области технологий к этому и другим малоресурсным языкам.**


### **Выбор Источников Данных**

Выбор источников данных для сбора текстов на кабардинском языке был направлен на эффективность и максимальное покрытие. Основным критерием стало наличие обширного и доступного контента, что привело к выбору крупных новостных сайтов. Эти сайты предоставляют богатый массив текстов, что идеально подходит для начального этапа сбора данных.

В качестве основных источников были выбраны следующие ресурсы:

- **Электронная газета "Адыгэ Псалъэ"**: Один из ведущих источников, предоставляющий обширный массив новостей и статей на кабардинском языке.
- **Электронная газета "Кабардино-Балкария" (статьи на кабардинском языке)**: Ещё один ключевой ресурс, обогащающий нашу базу данных разнообразными текстами.
- **Журнал Iуащхьэмахуэ (Эльбрус)**: Журнал, который вносит вклад в разнообразие собираемого материала.

Эти источники были выбраны не только за их доступность, но и за способность предоставить разнообразный набор текстов, отражающих современное использование кабардинского языка. Такой подход позволяет собрать широкий спектр материалов - от новостных статей до культурных и образовательных публикаций.

### **Использование Scrapy для Сбора Данных**

#### Почему Scrapy?

Есть много достойных инструментов для веб-скрапинга [awesome-web-scraping](https://github.com/lorien/awesome-web-scraping?tab=readme-ov-file).
Да и наверное самым простым и быстрым способом собрать данные с веб-сайта является использование библиотеки `requests` + `BeautifulSoup` в Python.
Но для долгосрочных проектов веб-скрапинга, лучше использовать более мощные инструменты, которые обеспечивают эффективность и надежность сбора данных.

Для этого проекта я выбрал Scrapy, исходя из его способности эффективно масштабироваться и адаптироваться к растущим потребностям сбора данных. 
Scrapy выделяется своим богатым функционалом и предоставляет гибкие возможности для парсинга и обработки данных. 
Например встроенные механизмы кеширования запросов и ответов, настройкой экспорта данных в различные форматы, а также управлением интенсивностью обхода веб-сайтов без написания дополнительного кода.
К тому же Scrapy с его удобным API для расширения, не ограничивает возможности тонкой настройки и расширения функционала.
Это делает его хорошим выбором для долгосрочных проектов веб-скрапинга.


#### Основные Шаги Настройки

1. **Кеширование Запросов и Ответов**: Одной из важных настроек в Scrapy является механизм кеширования запросов и ответов. 
Это особенно полезно при работе с новостными сайтами, где контент обычно статичен и не обновляется часто. 
Кеширование позволяет избежать повторных запросов при последующих обходах, снижая нагрузку на сайты и ускоряя процесс продолжения или повторного запуска обхода. 
```python
# settings.py

HTTPCACHE_ENABLED = True
HTTPCACHE_EXPIRATION_SECS = 0
HTTPCACHE_DIR = "httpcache"
HTTPCACHE_STORAGE = "scrapy.extensions.httpcache.FilesystemCacheStorage"
```

2. **Управление Интенсивностью Обхода**: более щадящий режим работы скрапера, чтобы минимизировать нагрузку на веб-сайты, уважая эти ресурсы и не нарушая их работу. 
Ну и избежание потенциальных блокировок со стороны веб-сайтов.
```python
# settings.py

DOWNLOAD_DELAY = 0.25
CONCURRENT_REQUESTS_PER_DOMAIN = 4

AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 5
AUTOTHROTTLE_MAX_DELAY = 60
AUTOTHROTTLE_DEBUG = True
```


### **Примеры Кода**

Пример кода паука Scrapy, который был использован для сбора данных с сайта [apkbr.ru](http://www.apkbr.ru/):

```python
import os

from scrapy.exporters import JsonLinesItemExporter
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


class ApkbrRuSpider(CrawlSpider):
    name = 'apkbr_ru'
    allowed_domains = ['apkbr.ru']
    start_urls = ['https://apkbr.ru/']

    rules = (
        Rule(LinkExtractor(allow=r'/node?page=\d+'), follow=True),
        Rule(LinkExtractor(allow=r'/calendar/.+'), follow=True),
        Rule(LinkExtractor(allow=r'/node/\d+'), callback='parse_item', follow=True),
    )

    def __init__(self, *args, **kwargs):
        super(ApkbrRuSpider, self).__init__(*args, **kwargs)
        self.dump_dir = f'../data/{self.name}'
        os.makedirs(self.dump_dir, exist_ok=True)

        self.file = open(os.path.join(self.dump_dir, 'apkbr_ru.jl'), 'wb')
        self.exporter = JsonLinesItemExporter(self.file, encoding='utf-8', ensure_ascii=False)
        self.exporter.start_exporting()

    def close_spider(self, spider):
        self.exporter.finish_exporting()
        self.file.close()

    def parse_item(self, response):
        title = response.css('h1.title::text').get()
        publication_date = response.css('div.meta.submitted span::attr(content)').get()
        content = ''.join(response.css('div.field-name-body ::text').getall())
        author = response.css('div.field-name-field-author .field-item::text').get()

        item = {
            'url': response.url,
            'title': title.strip() if title else None,
            'publication_date': publication_date.strip() if publication_date else None,
            'content': content.strip() if content else None,
            'author': author.strip() if author else None,
        }

        self.exporter.export_item(item)
        return item

```

1. **Настройка и Инициализация**: Создание директории для данных, открытие файла для сохранения данных в формате JSON Lines.

2. **Правила Обхода**: Определение правил для LinkExtractor, чтобы указать, какие ссылки следует посещать.

```python
rules = (
    Rule(LinkExtractor(allow=r'/node?page=\d+'), follow=True),  # обход пагинации
    Rule(LinkExtractor(allow=r'/calendar/.+'), follow=True),  # обход календаря
    Rule(LinkExtractor(allow=r'/node/\d+'), callback='parse_item', follow=True),  # обход и извлечение данных
)
```
Нам не нужны парсить все страницы, поэтому мы ограничиваемся только теми, которые содержат новости.
Это страницы с ссылками вида `/node/1234`.
Но обход пагинации и календаря также важен, поскольку он позволяет не пропустить страницы с новостями.  

3. **Парсинг Страниц**: Функция `parse_item` извлекает необходимые данные с веб-страницы, включая заголовок, дату публикации, содержимое и автора.

4. **Экспорт Данных**: Использование `JsonLinesItemExporter` для структурирования и сохранения данных в удобочитаемом формате.

Важно собрать структурированные данные, чтобы облегчить их дальнейшее использование и анализ.

### **Структура данных**

Собранные данные были сохранены в формате JSON Lines, который представляет собой последовательность строк, каждая из которых содержит отдельный JSON-объект. Это удобный формат для хранения структурированных данных, поскольку он позволяет легко извлекать и использовать данные.

Пример структуры данных:
```json
{
    "url": "https://apkbr.ru/node/1234",
    "title": "Заголовок Новости",
    "publication_date": "2021-01-01T00:00:00+03:00",
    "content": "Текст Новости",
    "author": "Автор Новости"
}
```

Наибольший интерес представляет поле `content`, которое содержит текст новостей и статей. 
Это основной материал для дальнейшего анализа и исследований.
Но остальные поля также могут быть полезны для дополнительного анализа или использования в качестве метаданных.


### **Ссылка на Репозиторий проекта для сбора данных**
- **Github проекта**: [zbze_crawler](https://github.com/panagoa/zbze_crawler/)
- **Использование проекта**: 
    - Склонируйте репозиторий: `git clone git@github.com:panagoa/zbze_crawler.git`
    - Установите зависимости: `pip install -r requirements.in`
    - Запустите паука: `scrapy crawl apkbr_ru` из директории `zbze_scrapy`
- **Структура собранных данных**:
```
data
├── apkbr_ru
│   ├── apkbr_ru.jl
├── apkbr_ru_rss
│   ├── apkbr_ru_rss.jl
├── elgkbr_ru
│   ├── elgkbr_ru.jl
└── oshhamaho
    ├── 01-2011.pdf
    ├── 01-2013.pdf
```

### **Заключение**

С этой статьей мы начинаем важное путешествие по сохранению и развитию кабардинского языка в цифровую эпоху. 
Мы не просто рассмотрели базовый процесс сбора данных, но и открыли дверь к новым возможностям и подходам в исследовании малоресурсных языков. Это исследование — не просто техническая задача, но и культурный проект, который помогает сохранить и обогатить уникальное языковое наследие.

Я приглашаю вас углубиться в тему сбора и анализа данных с помощью дополнительных материалов, доступных в разделе [Ссылки на Дополнительные Материалы](#ссылки-на-дополнительные-материалы). 
В следующих статьях мы продолжим наше путешествие, сосредоточив внимание на анализе собранных данных и извлечении из них полезной информации. 
Вместе мы сможем не только сохранить, но и обогатить кабардинский язык, давая ему новую жизнь в современном мире.


### **Как Вы Можете Помочь**

Ваш активный вклад и обратная связь играют ключевую роль в повышении качества и доступности данных для исследований и разработки. 

- **Улучшение кода**: Если у вас есть опыт в программировании, помогите улучшить код нашего проекта, предлагая исправления или новые функции. Ваш технический вклад поможет сделать наш инструмент более мощным и доступным для других исследователей.

- **Знание кабардинского языка и культуры**: Если вы знакомы с кабардинским языком или культурой, поделитесь идеями по расширению нашей базы данных или улучшению точности сбора данных. Ваше знание языка и культуры является ценным ресурсом для обогащения нашего проекта.

Каждый ваш вклад, будь то маленький совет или большой кусок кода, приближает нас к сохранению кабардинского языка и его интеграции в современный цифровой мир. Работая вместе, мы не только сохраняем языковое наследие, но и создаем новые возможности для его изучения и использования в будущем.

### Ссылки на Дополнительные Материалы
- [Scrapy documentation](https://docs.scrapy.org/en/latest/)
- [Introducing Scrapy: The Powerful Python Library For Efficient Web Scraping](https://medium.com/aws-tip/introducing-scrapy-the-powerful-python-library-for-efficient-web-scraping-ef4557ec6d2)
- [How-to: Build a Python Web Scraper to capture IMDb Top-100 Movies](https://medium.com/@abdulrwahab/how-to-build-a-python-web-scraper-to-capture-imdb-top-100-movies-908bf9b6bc19)
- [awesome-web-scraping](https://github.com/lorien/awesome-web-scraping?tab=readme-ov-file)