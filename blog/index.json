{
  "../blog/00_base/08_future_of_the_project.md": {
    "filename": "08_future_of_the_project.md",
    "path": "../blog/00_base/08_future_of_the_project.md",
    "relative_path": "00_base/08_future_of_the_project.md",
    "title": "Без названия",
    "hash": "fc3934bf4a02d43ef4773acf9e87f0bc",
    "is_updated": false,
    "content": "В этом разделе обсуждаются планы и направления дальнейшего развития проекта, включая расширение исследований, разработку новых инструментов и методик.",
    "summary": "**08_future_of_the_project.md** - раздел, в котором обсуждаются планы и перспективы дальнейшего развития проекта. В тексте затрагиваются темы расширения исследований, введения новых инструментов и методик. Настоящий материал представляет интерес для тех, кто хочет узнать о направлениях будущего прогресса проекта."
  },
  "../blog/00_base/01_about_kabardian_language.md": {
    "filename": "01_about_kabardian_language.md",
    "path": "../blog/00_base/01_about_kabardian_language.md",
    "relative_path": "00_base/01_about_kabardian_language.md",
    "title": "Без названия",
    "hash": "fdf8107cd1e5f376a9af402b3451658f",
    "is_updated": false,
    "content": "В этой статье представлен обзор кабардинского языка, его истории, уникальных характеристик и места в культурном и лингвистическом многообразии. Рассматривается значимость языка с точки зрения его сохранения и изучения в современном мире.",
    "summary": "```markdown\nСтатья '../blog/00_base/01_about_kabardian_language.md' представляет собой краткий обзор кабардинского языка. Текст знакомит с историей языка, его уникальными особенностями, а также освещает его значение в культурном и лингвистическом контекстах. Также в статье поднимается тема сохранения и изучения кабардинского языка в современном мире.\n```"
  },
  "../blog/00_base/07_contacts_and_collaboration.md": {
    "filename": "07_contacts_and_collaboration.md",
    "path": "../blog/00_base/07_contacts_and_collaboration.md",
    "relative_path": "00_base/07_contacts_and_collaboration.md",
    "title": "Без названия",
    "hash": "0db964bafe0aef2da876347755752848",
    "is_updated": false,
    "content": "Эта страница предлагает информацию о том, как связаться с автором проекта, и приглашает к сотрудничеству, обмену знаниями и участию в проекте.",
    "summary": "```markdown\nСтраница '../blog/00_base/07_contacts_and_collaboration.md' содержит контактную информацию автора блога и рассказывает о возможностях сотрудничества, обмена знаниями и участии в развитии проекта. Эта информация будет полезна для читателей, желающих принять активное участие в жизни блога, делиться своими знаниями или внести свой вклад в развитие проекта.\n```\n"
  },
  "../blog/00_base/02_project_motivation_and_goals.md": {
    "filename": "02_project_motivation_and_goals.md",
    "path": "../blog/00_base/02_project_motivation_and_goals.md",
    "relative_path": "00_base/02_project_motivation_and_goals.md",
    "title": "Без названия",
    "hash": "ebb9f62df1e9e7cfe3f6c98f34b1d7fe",
    "is_updated": false,
    "content": "Здесь описывается личная мотивация за созданием блога, цели проекта и его потенциальный вклад в область NLP. Обсуждаются личные причины исследования кабардинского языка и ожидаемые результаты.",
    "summary": "```markdown\nФайл '../blog/00_base/02_project_motivation_and_goals.md' содержит информацию о мотивации автора для создания этого блога и описание целей данного проекта, который освещает применение методов NLP к исследованию кабардинского языка. В тексте файла раскрыты личные мотивы автора при выборе темы и ожидаемые результаты работы.\n```"
  },
  "../blog/00_base/06_research_opportunities.md": {
    "filename": "06_research_opportunities.md",
    "path": "../blog/00_base/06_research_opportunities.md",
    "relative_path": "00_base/06_research_opportunities.md",
    "title": "Без названия",
    "hash": "bf72ae635c2af14beeae459da5cc1e3c",
    "is_updated": false,
    "content": "Раздел посвящен потенциалу проекта для научных исследований, обсуждению возможностей для лингвистов, исследователей и разработчиков, заинтересованных в кабардинском языке и NLP.",
    "summary": "```markdown\nФайл '../blog/00_base/06_research_opportunities.md' содержит информацию о потенциале проекта для научных исследований, включая возможности для лингвистов, исследователей и разработчиков, интересующихся кабардинским языком и обработкой естественного языка (NLP). Особое внимание уделяется возможностям использования проекта в качестве ресурса для изучения и разработки в области кабардинского языка и NLP.\n```"
  },
  "../blog/00_base/03_data_collection_and_sources.md": {
    "filename": "03_data_collection_and_sources.md",
    "path": "../blog/00_base/03_data_collection_and_sources.md",
    "relative_path": "00_base/03_data_collection_and_sources.md",
    "title": "Без названия",
    "hash": "5f71da62129df22d256e13180bf8b705",
    "is_updated": false,
    "content": "Эта страница посвящена методам и стратегиям сбора данных для анализа кабардинского языка, включая описание источников, таких как онлайн-публикации, архивы и цифровые библиотеки.",
    "summary": "```markdown\nСтраница '../blog/00_base/03_data_collection_and_sources.md' подробно рассматривает методики и стратегии сбора данных для анализа кабардинского языка. В ней описываются различные источники данных, включая онлайн-публикации, архивы и цифровые библиотеки. Ресурс будет полезен для исследователей и любого, кто интересуется изучением языков.\n```"
  },
  "../blog/00_base/05_copyrights_and_usage_terms.md": {
    "filename": "05_copyrights_and_usage_terms.md",
    "path": "../blog/00_base/05_copyrights_and_usage_terms.md",
    "relative_path": "00_base/05_copyrights_and_usage_terms.md",
    "title": "Без названия",
    "hash": "163687bbbeb0352313a26dd0d3491702",
    "is_updated": false,
    "content": "На этой странице рассматриваются юридические аспекты использования данных, включая авторские права, условия использования материалов и соответствующие лицензии.",
    "summary": "```markdown\n[Юридические аспекты использования данных](../blog/00_base/05_copyrights_and_usage_terms.md): эта страница посвящена правовым аспектам использования данных. Она охватывает вопросы авторских прав, условий использования материалов и связанных с ними лицензий. Рекомендуется для прочтения всем, кто использует данные в своей работе или учебе.\n```"
  },
  "../blog/00_base/04_data_processing_and_structure.md": {
    "filename": "04_data_processing_and_structure.md",
    "path": "../blog/00_base/04_data_processing_and_structure.md",
    "relative_path": "00_base/04_data_processing_and_structure.md",
    "title": "Без названия",
    "hash": "1838560b415d2f434ab68b7587620744",
    "is_updated": false,
    "content": "Здесь обсуждаются технические аспекты обработки собранных данных: очистка, структурирование, подготовка данных к анализу и применяемые инструменты и технологии.",
    "summary": "```markdown\nФайл '../blog/00_base/04_data_processing_and_structure.md' освещает важные технические аспекты работы с данными, включая их очистку, структурирование и подготовку к анализу. В документе также рассматриваются используемые инструменты и технологии, что делает его полезным ресурсом для всех, кто занимается обработкой и анализом данных.\n```"
  },
  "../blog/02_snippets/auto_generated/python/snippet_generator.md": {
    "filename": "snippet_generator.md",
    "path": "../blog/02_snippets/auto_generated/python/snippet_generator.md",
    "relative_path": "02_snippets/auto_generated/python/snippet_generator.md",
    "title": "Объяснение кода из файла `snippet_generator.py`",
    "hash": "dc9351caabda478b9c77d1fd1bfc554c",
    "is_updated": false,
    "content": "# Объяснение кода из файла `snippet_generator.py`\n\nДанный код предназначен для генерации сниппетов (небольших фрагментов) кода, в которых объясняется функционал определенного файла. Для генерации используется API сервиса OpenAI и его модель \"gpt-4\".\n\n## Структура кода\n\nПервоначально инициализируется клиент сервиса OpenAI.\n\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n```\n\nЗатем определяется функция `generate_snippet`, которая принимает два аргумента: путь к файлу с исходным кодом и путь к выходному файлу, в который будет записан сгенерированный сниппет.\n\n```python\ndef generate_snippet(file_path, output_path):\n    # код функции\n```\n\nВнутри функции исходный код считывается из файла, после чего создается запрос к API OpenAI, в котором формируется сообщение об описании требуемого сниппета.\n\n```python\n    with open(file_path, 'r') as file:\n        code = file.read()\n\n    if not code:\n        raise ValueError(f\"File {file_path} is empty\")\n\n    response = client.chat.completions.create(\n        messages=[\n            {\"role\": \"user\",\n            \"content\": (\n                f\"текст сообщения с описанием требований и сниппетом кода\"\n                f\"\\n\\n\"\n                f\"```python\\n{code}\\n```\\n\\n\")},\n            ],\n        model=\"gpt-4\",\n    )\n```\n\nПолученный результат от API OpenAI сохраняется в файл.\n\n```python\n    snippet = response.choices[0].message.content\n    markdown_snippet = f\"{snippet}\"\n\n    with open(output_path, 'w') as output_file:\n        output_file.write(markdown_snippet)\n```\n\nВ конце файла осуществляется проверка на количество аргументов командной строки и вызов функции `generate_snippet` с нужными аргументами.\n\n```python\nif __name__ == \"__main__\":\n    if len(sys.argv) != 3:\n        print(\"Usage: python snippet_generator.py [source_file_path] [output_markdown_path]\")\n        sys.exit(1)\n\n    source_file_path = sys.argv[1]\n    output_markdown_path = sys.argv[2]\n\n    generate_snippet(source_file_path, output_markdown_path)\n```\n\nТаким образом, данный Python скрипт позволяет автоматически генерировать короткие и информативные сниппеты, содержащие объяснения исходного кода файла, используя API сервиса OpenAI.",
    "summary": "```markdown\n# Краткие фрагменты кода с объяснением их функционала через API OpenAI\n\nФайл `snippet_generator.md` содержит руководство по использованию Python-скрипта для автоматической генерации сниппетов кода из исходного файла. Сниппеты кода служат для быстрого понимания функционала конкретного участка кода. Для создания объяснений используется API сервиса OpenAI и его модель \"gpt-4\". В файле подробно описывается структура кода и приводятся примеры его реализации с вставками исходного кода, вклчающие инициализацию API, определение функций для чтения исходника и генерации сниппета, формирование запроса к API, сохранение результата в файл и проверку на количество аргументов командной строки.\n```\n"
  },
  "../blog/02_snippets/auto_generated/python/text_cleaner.md": {
    "filename": "text_cleaner.md",
    "path": "../blog/02_snippets/auto_generated/python/text_cleaner.md",
    "relative_path": "02_snippets/auto_generated/python/text_cleaner.md",
    "title": "Приведение текстовых данных к нужному виду с помощью Python",
    "hash": "1581098e21940c2140e442f3df231cb9",
    "is_updated": false,
    "content": "# Приведение текстовых данных к нужному виду с помощью Python\n\nВ работе с текстовыми данными часто приходится сталкиваться с необходимостью их очистки и препроцессинга. Представленный в этом сниппете код на языке Python позволяет это выполнить. Давайте взглянем на основные функции и объясним, что они делают.\n\n### Основные идеи при очистке текста\n\n- Удаление лишних символов\n- Замена повторяющихся единиц текста\n- Объединение слов, разделенных дефисами\n- Удаление \"мусорного\" текста\n\n### Процедуры очистки\n\nВо-первых, мы импортируем две библиотеки: `re` – для работы с регулярными выражениями и `regex`, который даёт больше возможностей по работе с Юникод символами.\n\n```python\nimport re\nimport regex\n```\n\nЗдесь у нас определен регулярное выражение, которое исключает все, что не входит в \"белый список\". Этот список включает в себя кириллицу, латиницу, некоторые спецсимволы и цифры. \n\n```python\nNOT_IN_WHITELIST_REGEX = r\"[^АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯяIi1\\-\\.\\,\\:\\; \\\n-!?–…«»1234567890)(№*×><]+\"\n```\n\nСледующие функции это набор различных очисток:\n\n- Удаление текстовых фрагментов с кодировками наборов символов `(cid:...)`\n- Удаление указаний секций `(i:...)`\n- Удаление слов, написанных заглавными буквами\n- Замена множественных новых строк на одну\n- Удаление пробелов перед знаками пунктуаций\n- Удаление слов с `II`\n- Добавление пробела после запятой\n- Удаление наборов больше одного пробела\n- Удаление лишних точек\n\nПример:\n\n```python\ndef remove_uppercase_words(text):\n    return re.sub(r\"\\b[А-ЯЁA-Z]{5,}\\b\", \"\", text)\n```\n\nЭта функция принимает на вход строку текста, затем использует функцию `sub` из библиотеки `re` для замены регулярному выражению (`\\b[А-ЯЁA-Z]{5,}\\b`, что является любым словом из 5 или более заглавных букв) на пустую строку.\n\nВ конце есть функция `clean_text`, которая объединяет весь процесс очистки. Она вызывает каждую функцию очистки по очереди на входном тексте.\n\n```python\ndef clean_text(text):\n    if not text:\n        return \"\"\n\n    clean_functions = [\n        remove_non_whitelisted_chars,\n        remove_html_tags,\n        ...\n        unify_hyphenated_words,\n    ]\n\n    for func in clean_functions:\n        text = func(text)\n\n    return text\n```\n\nВ остальном, в тексте удаляются HTML теги, эмодзи, повторяющаяся пунктуация и \"мусорный\" текст. \n\nТаким образом, данный код эффективно и быстро обработает большие тексты, оставляя только значимую информацию.",
    "summary": "```markdown\n# Описание\nВ файле 'text_cleaner.md' рассказывается о полезных методах в Python для очистки и препроцессинга текстовых данных. В статье даются советы, как импортировать необходимые библиотеки, удалить лишние символы, заменить повторяющиеся единицы текста, объединить слова, разделенные дефисами, и очистить текст от \"мусора\". Подробно разъясняются такие процедуры очистки, как удаление фрагментов с кодировками наборов символов, указаний секций, слов заглавными буквами и лишних точек, замена множественных новых строк на одну и прочее. Отдельное внимание уделено использованию регулярных выражений для работы с текстом. В конце приводится общая функция `clean_text`, объединяющая все методы очистки.\n```"
  },
  "../blog/02_snippets/auto_generated/notebooks/02_00_text_analyze_bigram_extract.md": {
    "filename": "02_00_text_analyze_bigram_extract.md",
    "path": "../blog/02_snippets/auto_generated/notebooks/02_00_text_analyze_bigram_extract.md",
    "relative_path": "02_snippets/auto_generated/notebooks/02_00_text_analyze_bigram_extract.md",
    "title": "Анализ текста и извлечение биграмм с помощью Python и NLTK",
    "hash": "0a44edcdb5c24239e20a4aa65653269c",
    "is_updated": false,
    "content": "# Анализ текста и извлечение биграмм с помощью Python и NLTK\n\nДанный код позволяет исследовать связь между словами (более конкретно биграммами) в заданном текстовом файле. \nБиграмма – это пара последовательных слов в тексте.\n\n## Чтение данных\nМы начинаем со стандартного импорта библиотек. Путь до файла указывается в переменной `file_path`. `BUF_SIZE` параметр, который контролирует сколько символов мы читаем из файла за одну итерацию. \n```python\nimport os\nfrom collections import Counter\nimport nltk\n\nfile_dir = '../data/processed'\nfile_name = 'oshhamaho.txt'\nfile_path = os.path.join(file_dir, file_name)\nBUF_SIZE = 100000\n```\n\n## Извлечение биграмм\n```python\ncnt = Counter()\nwith open(file_path) as f:\n    tmp_raw = f.read(BUF_SIZE)\n    while tmp_raw:\n        tokens = nltk.word_tokenize(tmp_raw)\n        text = nltk.Text(tokens)\n        clc = text.collocation_list(5000)\n        cnt.update(clc)\n        tmp_raw = f.read(BUF_SIZE)\n```\n`nltk.word_tokenize` разбивает текст на отдельные слова. `nltk.Text` представляет текст как список слов. Метод `collocation_list` выдает наиболее частые биграмы текста. Результат сохраняется в `cnt`, который является объектом счетчика слов.\n\n## Создание DataFrame\n```python\nimport pandas as pd\n\ndata = [\n    {\n        'w_bigram': ' '.join(bigram),\n        'freq': freq\n    }\n    for bigram, freq in cnt.items()\n    if freq > 2\n]\n\ndf = pd.DataFrame(data)\ndf = df.sort_values('freq', ascending=False)\n```\nЭти данные затем преобразуются в DataFrame для удобства просмотра и дальнейшей обработки.\n\n## Сохранение данных\n```python\ndf.to_csv(\n    os.path.join('../data/processed/word_bigrams', file_name.replace('.txt', '.csv')),\n    index=False,\n)\n```\nDataFrame сохраняется в CSV файл.\n\n## Проверка вероятности последовательности биграмм\n```python\nfrom nltk.probability import ConditionalFreqDist, ConditionalProbDist, MLEProbDist\n\ncfdist = ConditionalFreqDist()\n\nfor bigram, freq in cnt.items():\n    word1, word2 = bigram\n    cfdist[word1][word2] = freq\n\ncpdist = ConditionalProbDist(cfdist, MLEProbDist)\n```\nЗдесь мы проверяем, насколько вероятно, что одно слово следует за другим.\n\n## Получение вероятности для каждого слова\n```python\ndef get_prob(word1):\n    prob_dict = {}\n    for word_2 in cpdist[word1].samples():\n        prob_dict[word_2] = cpdist[word1].prob(word_2) * 100\n    return prob_dict\n\nsorted(get_prob('псалъэ').items(), key=lambda x: x[1], reverse=True)\n```\n`get_prob` возвращает словарь, показывающий, какие слова наиболее вероятно идут после данного слова. \n\nТакими способами мы можем проводить простой и исчерпывающий анализ текста. Код довольно прост, но позволяет получить полезную информацию о структуре текста и взаимосвязи слов в этом тексте.",
    "summary": "# Анализ текста и извлечение биграмм с Python и NLTK\n\nВ данном `Markdown` файле описывается процесс анализа текстовых данных и извлечения из него биграмм - пар последовательных слов - с помощью библиотеки Python для обработки текстов `NTLK`. Описываются этапы чтения данных, извлечение биграмм, а также создание и сохранение `DataFrame` библиотеки `pandas` с полученными данными для дальнейшей обработки. Также приводятся примеры проверки вероятности последовательности биграмм и получение вероятности для каждого слова. Данный файл является полезным ресурсом для тех, кто хочет провести полноценный анализ структуры текста и взаимосвязи слов с использованием Python."
  },
  "../blog/02_snippets/auto_generated/notebooks/00_clean_scraped_text.md": {
    "filename": "00_clean_scraped_text.md",
    "path": "../blog/02_snippets/auto_generated/notebooks/00_clean_scraped_text.md",
    "relative_path": "02_snippets/auto_generated/notebooks/00_clean_scraped_text.md",
    "title": "Очистка и предобработка текста",
    "hash": "8d73c66c44a69974341e30a29d9afa86",
    "is_updated": false,
    "content": "# Очистка и предобработка текста\n\nВ этом модуле Python мы будем использовать numpy, pandas и os для считывания, очистки и сохранения данных, полученных в результате web-скрейпинга.\n\n```python\nimport os  # библиотека для работы с файлами и папками\nimport pandas as pd  # библиотека для работы с таблицами данных\nfrom src.text_cleaner import clean_text  # функция для очистки текста\n```\n\nЭтот код используется для импорта необходимых библиотек и функций.\n\n```python\nexternal_dir = '../data/external/'\ninterim_dir = '../data/interim/'\nos.makedirs(interim_dir, exist_ok=True)\n\nprocessed_dir = '../data/processed/'\nos.makedirs(processed_dir, exist_ok=True)\n```\n\nМы создаем папку для сохранения данных после очистки.\n\n```python\nfor file_name in ['apkbr_ru.jsonl', 'elgkbr_ru.jsonl']:    \n    df = pd.read_json(os.path.join(external_dir, file_name), lines=True)\n    df['content'] = df['content'].apply(clean_text)\n```\n\nЭтот код обрабатывает файлы json. Он загружает каждый файл в DataFrame (df), затем применяет функцию clean_text к каждой строке в колонке 'content'.\n\n```python\ndf.to_json(\n        os.path.join(interim_dir, file_name), \n        orient='records', \n        lines=True,\n        force_ascii=False\n    )\n```\n\nПосле очистки данные сохраняются обратно в json-файл.\n\n```python\ntext = '\\n'.join(df['content'].tolist())\n    with open(os.path.join(processed_dir, file_name.replace('.jsonl', '.txt')), 'w') as f:\n        f.write(text)\n```\n\nЗдесь содержимое каждой строки объединяется в одну большую строку, которая затем сохраняется в файл .txt. \n\nИтак, этот сниппет берет необработанные данные, полученные из веб-скрапинга, и чистит их от нестандартных символов и пробелов, чтобы подготовить их к дальнейшей обработке. После очистки данные сохраняются в двух форматах - .json и .txt.",
    "summary": "```markdown\n# Очистка и предобработка текста после веб-скрейпинга\n\nMarkdown-файл '../blog/02_snippets/auto_generated/notebooks/00_clean_scraped_text.md' описывает процессы чтения, очистки и сохранения данных, полученных в результате web-скрейпинга, с помощью модуля Python. Показан пример использования библиотек `numpy`, `pandas` и `os` для управления данными и файлами. Отдельное внимание уделено работе с файлами в формате JSON и запуску функции очистки текста `clean_text`. После обработки данные сохраняются в формате .json и .txt. Этот материал будет полезным для тех, кто изучает предобработку данных для последующего анализа текста.\n```\n"
  },
  "../blog/02_snippets/auto_generated/notebooks/01_extract_text_from_pdf.md": {
    "filename": "01_extract_text_from_pdf.md",
    "path": "../blog/02_snippets/auto_generated/notebooks/01_extract_text_from_pdf.md",
    "relative_path": "02_snippets/auto_generated/notebooks/01_extract_text_from_pdf.md",
    "title": "Без названия",
    "hash": "92c822ffa214434c972bb610645283f3",
    "is_updated": false,
    "content": "## Извлечение текста из PDF файла с помощью Python\n\nPython обладает множеством библиотек, с помощью которых можно обрабатывать различные типы файлов. В данном сниппете мы рассмотрим как извлечь текст из PDF файлов и сохранить его в txt файлы с помощью модулей `textract`, `os` и `tqdm`.\n\nПрежде всего, с помощью модуля `os` создаётся папка для сохранения текстовых файлов.\n\n```python\noshhamaho_external_pdf_dir = '../data/external/oshhamaho'\noshhamaho_interim_txt_dir = '../data/interim/oshhamaho'\nos.makedirs(oshhamaho_interim_txt_dir, exist_ok=True)\n\npdf_files = [f for f in os.listdir(oshhamaho_external_pdf_dir) if f.endswith('.pdf')]\n```\n\nЗатем определена функция `extract_text_from_pdf_file()`, которая берёт на вход путь к PDF файлу, извлекает из него текст и возвращает этот текст в виде строки, с использованием библиотеки `textract`.\n\n```python\nimport textract\ndef extract_text_from_pdf_file(pdf_path):\n    try:\n        text = textract.process(pdf_path, method='pdfminer')\n    except Exception:\n        return ''\n    text = text.decode('utf-8')\n    return text\n```\n\nЦиклом пробегаем по всем файлам с расширением '.pdf', извлекаем текст и сохраняем как текстовый файл '.txt'.\n\n```python\npgbar = tqdm(pdf_files)\n\nfor pdf_file in pgbar:\n    pgbar.set_description(pdf_file)\n    pdf_path = os.path.join(oshhamaho_external_pdf_dir, pdf_file)\n    \n    text_i = extract_text_from_pdf_file(pdf_path)\n    cleaned_text = clean_text(text_i) \n\n    text_file = pdf_file.replace('.pdf', '.txt')\n    text_path = os.path.join(oshhamaho_interim_txt_dir, text_file)\n    \n    with open(text_path, 'w') as f:\n        f.write(cleaned_text)\n```\n\nВ данном коде используется `tqdm` для создания интерактивной строки прогресса. Это не обязательно, но увеличивает удобство работы.\n\nЗатем из каждого текстового файла текст собирается, очищается и склеивается в одну большую строку, после чего записывается в новый файл.\n\n```python\ntxt_files = sorted([f for f in os.listdir(oshhamaho_interim_txt_dir) if f.endswith('.txt')])\nprocessed_dir = '../data/processed/'\nos.makedirs(processed_dir, exist_ok=True)\n\noshhamaho_all_txt_path = os.path.join(processed_dir, 'oshhamaho.txt')\n\npgbar = tqdm(txt_files)\ntext = ''\n\nfor txt_file in pgbar:\n    pgbar.set_description(txt_file)\n    txt_path = os.path.join(oshhamaho_interim_txt_dir, txt_file)\n    \n    with open(txt_path, 'r') as f:\n        text_i = f.read()\n    \n    cleaned_text = clean_text(text_i) \n    text += cleaned_text + '\\n'\n\nwith open(oshhamaho_all_txt_path, 'w') as f:\n    f.write(text)\n```\nЭтот код можно использовать для автоматизации процесса извлечения текста из большого количества PDF файлов. Он легко модифицируется в соответствии с требованиями вашего проекта.",
    "summary": "```markdown\nФайл `01_extract_text_from_pdf.md` представляет из себя подробную инструкцию по извлечению текста из PDF файлов и сохранению этого текста в txt формате с использованием Python и модулей `textract`, `os` и `tqdm`. В статье разъясняются необходимые шаги для создания папки для сохранения текстовых файлов, определения функции `extract_text_from_pdf_file()`, которая извлекает текст из PDF и возвращает его в виде строки, и цикла обработки каждого PDF файла для извлечения и сохранения текста. Также демонстрируется использование строки прогресса `tqdm` для отслеживания хода обработки файлов. В конце объясняетса, как собрать все извлеченные тексты в единый файл. Эта инструкция может быть полезно применена для автоматизации процесса работы с большим объемом PDF файлов и может быть адаптирована под специфические требования каждого проекта.\n```"
  },
  "../blog/01_articles/01_kabardian_language_data_collection.md": {
    "filename": "01_kabardian_language_data_collection.md",
    "path": "../blog/01_articles/01_kabardian_language_data_collection.md",
    "relative_path": "01_articles/01_kabardian_language_data_collection.md",
    "title": "Кабардинский язык в Цифровом Мире: Этап Сбора Данных",
    "hash": "3b0ee867a52f8b19c8374aa32cad14d9",
    "is_updated": false,
    "content": "# Кабардинский язык в Цифровом Мире: Этап Сбора Данных \n\n### **Введение**\n\n**В эпоху цифровизации, когда мир становится все более глобализированным, многие малоресурсные языки, включая кабардинский, сталкиваются с угрозой исчезновения. Основная проблема этих языков – не уменьшение числа носителей, а недостаток цифровой поддержки и ограниченное присутствие в интернете, что в будущем может оказать серьезное влияние на их выживание. Кабардинский язык, богатый своей историей и культурой, заслуживает особого внимания в свете сохранения культурного наследия.**\n\n**Цель этой статьи - исследовать процесс сбора текстов на кабардинском языке из цифровых источников, таких как новостные сайты. Эти источники представляют собой ценный ресурс для анализа современного использования языка. В дальнейшем планируется изучение других методов сбора данных, включая материалы из социальных сетей и тексты книг.**\n\n**Выбор кабардинского языка для данного исследования обусловлен его значением как моего родного языка и стремлением сохранить его уникальное культурное наследие. Эта статья представляет собой шаг к цифровому возрождению кабардинского языка и привлечению внимания специалистов в области технологий к этому и другим малоресурсным языкам.**\n\n\n### **Выбор Источников Данных**\n\nВыбор источников данных для сбора текстов на кабардинском языке был направлен на эффективность и максимальное покрытие. Основным критерием стало наличие обширного и доступного контента, что привело к выбору крупных новостных сайтов. Эти сайты предоставляют богатый массив текстов, что идеально подходит для начального этапа сбора данных.\n\nВ качестве основных источников были выбраны следующие ресурсы:\n\n- **Электронная газета \"Адыгэ Псалъэ\"**: Один из ведущих источников, предоставляющий обширный массив новостей и статей на кабардинском языке.\n- **Электронная газета \"Кабардино-Балкария\" (статьи на кабардинском языке)**: Ещё один ключевой ресурс, обогащающий нашу базу данных разнообразными текстами.\n- **Журнал Iуащхьэмахуэ (Эльбрус)**: Журнал, который вносит вклад в разнообразие собираемого материала.\n\nЭти источники были выбраны не только за их доступность, но и за способность предоставить разнообразный набор текстов, отражающих современное использование кабардинского языка. Такой подход позволяет собрать широкий спектр материалов - от новостных статей до культурных и образовательных публикаций.\n\n### **Использование Scrapy для Сбора Данных**\n\n#### Почему Scrapy?\n\nЕсть много достойных инструментов для веб-скрапинга [awesome-web-scraping](https://github.com/lorien/awesome-web-scraping?tab=readme-ov-file).\nДа и наверное самым простым и быстрым способом собрать данные с веб-сайта является использование библиотеки `requests` + `BeautifulSoup` в Python.\nНо для долгосрочных проектов веб-скрапинга, лучше использовать более мощные инструменты, которые обеспечивают эффективность и надежность сбора данных.\n\nДля этого проекта я выбрал Scrapy, исходя из его способности эффективно масштабироваться и адаптироваться к растущим потребностям сбора данных. \nScrapy выделяется своим богатым функционалом и предоставляет гибкие возможности для парсинга и обработки данных. \nНапример встроенные механизмы кеширования запросов и ответов, настройкой экспорта данных в различные форматы, а также управлением интенсивностью обхода веб-сайтов без написания дополнительного кода.\nК тому же Scrapy с его удобным API для расширения, не ограничивает возможности тонкой настройки и расширения функционала.\nЭто делает его хорошим выбором для долгосрочных проектов веб-скрапинга.\n\n\n#### Основные Шаги Настройки\n\n1. **Кеширование Запросов и Ответов**: Одной из важных настроек в Scrapy является механизм кеширования запросов и ответов. \nЭто особенно полезно при работе с новостными сайтами, где контент обычно статичен и не обновляется часто. \nКеширование позволяет избежать повторных запросов при последующих обходах, снижая нагрузку на сайты и ускоряя процесс продолжения или повторного запуска обхода. \n```python\n# settings.py\n\nHTTPCACHE_ENABLED = True\nHTTPCACHE_EXPIRATION_SECS = 0\nHTTPCACHE_DIR = \"httpcache\"\nHTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n```\n\n2. **Управление Интенсивностью Обхода**: более щадящий режим работы скрапера, чтобы минимизировать нагрузку на веб-сайты, уважая эти ресурсы и не нарушая их работу. \nНу и избежание потенциальных блокировок со стороны веб-сайтов.\n```python\n# settings.py\n\nDOWNLOAD_DELAY = 0.25\nCONCURRENT_REQUESTS_PER_DOMAIN = 4\n\nAUTOTHROTTLE_ENABLED = True\nAUTOTHROTTLE_START_DELAY = 5\nAUTOTHROTTLE_MAX_DELAY = 60\nAUTOTHROTTLE_DEBUG = True\n```\n\n\n### **Примеры Кода**\n\nПример кода паука Scrapy, который был использован для сбора данных с сайта [apkbr.ru](http://www.apkbr.ru/):\n\n```python\nimport os\n\nfrom scrapy.exporters import JsonLinesItemExporter\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\n\n\nclass ApkbrRuSpider(CrawlSpider):\n    name = 'apkbr_ru'\n    allowed_domains = ['apkbr.ru']\n    start_urls = ['https://apkbr.ru/']\n\n    rules = (\n        Rule(LinkExtractor(allow=r'/node?page=\\d+'), follow=True),\n        Rule(LinkExtractor(allow=r'/calendar/.+'), follow=True),\n        Rule(LinkExtractor(allow=r'/node/\\d+'), callback='parse_item', follow=True),\n    )\n\n    def __init__(self, *args, **kwargs):\n        super(ApkbrRuSpider, self).__init__(*args, **kwargs)\n        self.dump_dir = f'../data/{self.name}'\n        os.makedirs(self.dump_dir, exist_ok=True)\n\n        self.file = open(os.path.join(self.dump_dir, 'apkbr_ru.jl'), 'wb')\n        self.exporter = JsonLinesItemExporter(self.file, encoding='utf-8', ensure_ascii=False)\n        self.exporter.start_exporting()\n\n    def close_spider(self, spider):\n        self.exporter.finish_exporting()\n        self.file.close()\n\n    def parse_item(self, response):\n        title = response.css('h1.title::text').get()\n        publication_date = response.css('div.meta.submitted span::attr(content)').get()\n        content = ''.join(response.css('div.field-name-body ::text').getall())\n        author = response.css('div.field-name-field-author .field-item::text').get()\n\n        item = {\n            'url': response.url,\n            'title': title.strip() if title else None,\n            'publication_date': publication_date.strip() if publication_date else None,\n            'content': content.strip() if content else None,\n            'author': author.strip() if author else None,\n        }\n\n        self.exporter.export_item(item)\n        return item\n\n```\n\n1. **Настройка и Инициализация**: Создание директории для данных, открытие файла для сохранения данных в формате JSON Lines.\n\n2. **Правила Обхода**: Определение правил для LinkExtractor, чтобы указать, какие ссылки следует посещать.\n\n```python\nrules = (\n    Rule(LinkExtractor(allow=r'/node?page=\\d+'), follow=True),  # обход пагинации\n    Rule(LinkExtractor(allow=r'/calendar/.+'), follow=True),  # обход календаря\n    Rule(LinkExtractor(allow=r'/node/\\d+'), callback='parse_item', follow=True),  # обход и извлечение данных\n)\n```\nНам не нужны парсить все страницы, поэтому мы ограничиваемся только теми, которые содержат новости.\nЭто страницы с ссылками вида `/node/1234`.\nНо обход пагинации и календаря также важен, поскольку он позволяет не пропустить страницы с новостями.  \n\n3. **Парсинг Страниц**: Функция `parse_item` извлекает необходимые данные с веб-страницы, включая заголовок, дату публикации, содержимое и автора.\n\n4. **Экспорт Данных**: Использование `JsonLinesItemExporter` для структурирования и сохранения данных в удобочитаемом формате.\n\nВажно собрать структурированные данные, чтобы облегчить их дальнейшее использование и анализ.\n\n### **Структура данных**\n\nСобранные данные были сохранены в формате JSON Lines, который представляет собой последовательность строк, каждая из которых содержит отдельный JSON-объект. Это удобный формат для хранения структурированных данных, поскольку он позволяет легко извлекать и использовать данные.\n\nПример структуры данных:\n```json\n{\n    \"url\": \"https://apkbr.ru/node/1234\",\n    \"title\": \"Заголовок Новости\",\n    \"publication_date\": \"2021-01-01T00:00:00+03:00\",\n    \"content\": \"Текст Новости\",\n    \"author\": \"Автор Новости\"\n}\n```\n\nНаибольший интерес представляет поле `content`, которое содержит текст новостей и статей. \nЭто основной материал для дальнейшего анализа и исследований.\nНо остальные поля также могут быть полезны для дополнительного анализа или использования в качестве метаданных.\n\n\n### **Ссылка на Репозиторий проекта для сбора данных**\n- **Github проекта**: [zbze_crawler](https://github.com/panagoa/zbze_crawler/)\n- **Использование проекта**: \n    - Склонируйте репозиторий: `git clone git@github.com:panagoa/zbze_crawler.git`\n    - Установите зависимости: `pip install -r requirements.in`\n    - Запустите паука: `scrapy crawl apkbr_ru` из директории `zbze_scrapy`\n- **Структура собранных данных**:\n```\ndata\n├── apkbr_ru\n│   ├── apkbr_ru.jl\n├── apkbr_ru_rss\n│   ├── apkbr_ru_rss.jl\n├── elgkbr_ru\n│   ├── elgkbr_ru.jl\n└── oshhamaho\n    ├── 01-2011.pdf\n    ├── 01-2013.pdf\n```\n\n### **Заключение**\n\nС этой статьей мы начинаем важное путешествие по сохранению и развитию кабардинского языка в цифровую эпоху. \nМы не просто рассмотрели базовый процесс сбора данных, но и открыли дверь к новым возможностям и подходам в исследовании малоресурсных языков. Это исследование — не просто техническая задача, но и культурный проект, который помогает сохранить и обогатить уникальное языковое наследие.\n\nЯ приглашаю вас углубиться в тему сбора и анализа данных с помощью дополнительных материалов, доступных в разделе [Ссылки на Дополнительные Материалы](#ссылки-на-дополнительные-материалы). \nВ следующих статьях мы продолжим наше путешествие, сосредоточив внимание на анализе собранных данных и извлечении из них полезной информации. \nВместе мы сможем не только сохранить, но и обогатить кабардинский язык, давая ему новую жизнь в современном мире.\n\n\n### **Как Вы Можете Помочь**\n\nВаш активный вклад и обратная связь играют ключевую роль в повышении качества и доступности данных для исследований и разработки. \n\n- **Улучшение кода**: Если у вас есть опыт в программировании, помогите улучшить код нашего проекта, предлагая исправления или новые функции. Ваш технический вклад поможет сделать наш инструмент более мощным и доступным для других исследователей.\n\n- **Знание кабардинского языка и культуры**: Если вы знакомы с кабардинским языком или культурой, поделитесь идеями по расширению нашей базы данных или улучшению точности сбора данных. Ваше знание языка и культуры является ценным ресурсом для обогащения нашего проекта.\n\nКаждый ваш вклад, будь то маленький совет или большой кусок кода, приближает нас к сохранению кабардинского языка и его интеграции в современный цифровой мир. Работая вместе, мы не только сохраняем языковое наследие, но и создаем новые возможности для его изучения и использования в будущем.\n\n### Ссылки на Дополнительные Материалы\n- [Scrapy documentation](https://docs.scrapy.org/en/latest/)\n- [Introducing Scrapy: The Powerful Python Library For Efficient Web Scraping](https://medium.com/aws-tip/introducing-scrapy-the-powerful-python-library-for-efficient-web-scraping-ef4557ec6d2)\n- [How-to: Build a Python Web Scraper to capture IMDb Top-100 Movies](https://medium.com/@abdulrwahab/how-to-build-a-python-web-scraper-to-capture-imdb-top-100-movies-908bf9b6bc19)\n- [awesome-web-scraping](https://github.com/lorien/awesome-web-scraping?tab=readme-ov-file)",
    "summary": "```markdown\n# Сбор данных на кабардинском языке для их дальнейшего анализа и использования в исследованиях \n\nЭтот Markdown документ через пример кабардинского языка иллюстрирует важность сохранения малоресурсных языков в цифровую эпоху. Основное внимание уделено подробному описанию процессу сбора и структурирования текстовых данных с таких цифровых источников, как новостные сайты. Продемонстрированы основные опции и функции библиотек Scrapy и BeautifulSoup для сбора данных, например, управление интенсивностью обхода и кеширование. Автор делится ссылкой на свой [Github проект](https://github.com/panagoa/zbze_crawler/) с кодом для сбора данных и перечисляет возможности для вклада в проект, как для программирующих, так и для носителей кабардинского языка. \n```\n"
  }
}