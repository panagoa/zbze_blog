{
  "../blog/00_base/08_future_of_the_project.md": {
    "filename": "08_future_of_the_project.md",
    "path": "../blog/00_base/08_future_of_the_project.md",
    "relative_path": "00_base/08_future_of_the_project.md",
    "hash": "fc3934bf4a02d43ef4773acf9e87f0bc",
    "is_updated": false,
    "content": "В этом разделе обсуждаются планы и направления дальнейшего развития проекта, включая расширение исследований, разработку новых инструментов и методик.",
    "title": "[Будущее и развитие проекта](../blog/00_base/08_future_of_the_project.md)",
    "summary": "Этот файл содержит обсуждение планов и стратегий для будущего развития проекта, включая углубление исследований и создание новых инструментов и методов."
  },
  "../blog/00_base/01_about_kabardian_language.md": {
    "filename": "01_about_kabardian_language.md",
    "path": "../blog/00_base/01_about_kabardian_language.md",
    "relative_path": "00_base/01_about_kabardian_language.md",
    "hash": "fdf8107cd1e5f376a9af402b3451658f",
    "is_updated": false,
    "content": "В этой статье представлен обзор кабардинского языка, его истории, уникальных характеристик и места в культурном и лингвистическом многообразии. Рассматривается значимость языка с точки зрения его сохранения и изучения в современном мире.",
    "title": "\"Обзор и значимость кабардинского языка\"",
    "summary": "Статья предлагает глубокий обзор кабардинского языка, анализируя его историю, уникальные характеристики и значение для современного мира, указывая на необходимость его сохранения и изучения.\n"
  },
  "../blog/00_base/07_contacts_and_collaboration.md": {
    "filename": "07_contacts_and_collaboration.md",
    "path": "../blog/00_base/07_contacts_and_collaboration.md",
    "relative_path": "00_base/07_contacts_and_collaboration.md",
    "hash": "0db964bafe0aef2da876347755752848",
    "is_updated": false,
    "content": "Эта страница предлагает информацию о том, как связаться с автором проекта, и приглашает к сотрудничеству, обмену знаниями и участию в проекте.",
    "title": "\"Контакты и приглашение к сотрудничеству\"",
    "summary": "Этот файл содержит информацию о том, как связаться с автором блога, а также предлагает возможность сотрудничества, обмена знаниями и участия в проекте."
  },
  "../blog/00_base/02_project_motivation_and_goals.md": {
    "filename": "02_project_motivation_and_goals.md",
    "path": "../blog/00_base/02_project_motivation_and_goals.md",
    "relative_path": "00_base/02_project_motivation_and_goals.md",
    "hash": "ebb9f62df1e9e7cfe3f6c98f34b1d7fe",
    "is_updated": false,
    "content": "Здесь описывается личная мотивация за созданием блога, цели проекта и его потенциальный вклад в область NLP. Обсуждаются личные причины исследования кабардинского языка и ожидаемые результаты.",
    "title": "\"Мотивация и цели создания блога по NLP\"",
    "summary": "Файл содержит описание личной мотивации автора для создания блога, целях данного проекта и его потенциальное значение в области обработки естественного языка. В нем также обсуждаются причины изучения кабардинского языка и ожидаемые результаты этого проекта."
  },
  "../blog/00_base/06_research_opportunities.md": {
    "filename": "06_research_opportunities.md",
    "path": "../blog/00_base/06_research_opportunities.md",
    "relative_path": "00_base/06_research_opportunities.md",
    "hash": "bf72ae635c2af14beeae459da5cc1e3c",
    "is_updated": false,
    "content": "Раздел посвящен потенциалу проекта для научных исследований, обсуждению возможностей для лингвистов, исследователей и разработчиков, заинтересованных в кабардинском языке и NLP.",
    "title": "\"Возможности проекта для научных исследований\"",
    "summary": "Этот файл содержит обсуждение возможностей проекта в контексте научных исследований, особенно для лингвистов, исследователей и разработчиков, работающих с кабардинским языком и областью обработки естественного языка (NLP)."
  },
  "../blog/00_base/03_data_collection_and_sources.md": {
    "filename": "03_data_collection_and_sources.md",
    "path": "../blog/00_base/03_data_collection_and_sources.md",
    "relative_path": "00_base/03_data_collection_and_sources.md",
    "hash": "5f71da62129df22d256e13180bf8b705",
    "is_updated": false,
    "content": "Эта страница посвящена методам и стратегиям сбора данных для анализа кабардинского языка, включая описание источников, таких как онлайн-публикации, архивы и цифровые библиотеки.",
    "title": "\"Методы сбора данных для анализа кабардинского языка\"",
    "summary": "Страница описывает различные методы и стратегии сбора данных для анализа кабардинского языка, включая подробное рассмотрение таких источников данных, как онлайн-публикации, архивы и цифровые библиотеки."
  },
  "../blog/00_base/05_copyrights_and_usage_terms.md": {
    "filename": "05_copyrights_and_usage_terms.md",
    "path": "../blog/00_base/05_copyrights_and_usage_terms.md",
    "relative_path": "00_base/05_copyrights_and_usage_terms.md",
    "hash": "163687bbbeb0352313a26dd0d3491702",
    "is_updated": false,
    "content": "На этой странице рассматриваются юридические аспекты использования данных, включая авторские права, условия использования материалов и соответствующие лицензии.",
    "title": "\"Юридические аспекты использования данных\"",
    "summary": "Страница посвящена юридическим нюансам использования данных, где обсуждаются вопросы авторских прав, условий использования контента и связанных с ним лицензий."
  },
  "../blog/00_base/04_data_processing_and_structure.md": {
    "filename": "04_data_processing_and_structure.md",
    "path": "../blog/00_base/04_data_processing_and_structure.md",
    "relative_path": "00_base/04_data_processing_and_structure.md",
    "hash": "1838560b415d2f434ab68b7587620744",
    "is_updated": false,
    "content": "Здесь обсуждаются технические аспекты обработки собранных данных: очистка, структурирование, подготовка данных к анализу и применяемые инструменты и технологии.",
    "title": "\"Технические аспекты обработки и структурирования данных\"",
    "summary": "Файл освещает технические аспекты работы со собранными данными, а именно: их очистку, структурирование и подготовку для дальнейшего анализа, при этом рассматриваются используемые инструменты и технологии."
  },
  "../blog/02_snippets/auto_generated/python/openai.md": {
    "filename": "openai.md",
    "path": "../blog/02_snippets/auto_generated/python/openai.md",
    "relative_path": "02_snippets/auto_generated/python/openai.md",
    "hash": "a084e43ed78b731f5b0df88bbb9e12e4",
    "is_updated": false,
    "content": "# Создание HTTP-клиента с использованием библиотеки OpenAI для взаимодействия с OpenAI API\n\nВ данном сниппете осуществляется создание HTTP-клиента, который позволяет взаимодействовать с OpenAI API. Это делается с помощью библиотеки openai.\n\nНиже приведен исходный код:\n\n```python\nimport os\nimport backoff\nimport httpx\nfrom openai import OpenAI, APIStatusError\n\ndef on_backoff(details):\n    print(details.__dict__)\n\nclass CustomHTTPClient(httpx.Client):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    @backoff.on_exception(\n        wait_gen=backoff.expo,\n        exception=Exception,\n        max_time=60,\n        max_tries=5,\n        jitter=backoff.full_jitter,\n        on_backoff=on_backoff\n    )\n    def send(self, request, *args, **kwargs):\n        print(f\"Requesting {args} {kwargs}\")\n        try:\n            response = super().send(request, *args, **kwargs)\n        except APIStatusError as e:\n            print(f\"APIStatusError: {e}\")\n            raise e\n        except Exception as e:\n            print(f\"Exception: {e}\")\n            raise e\n\n        return response\n\ndef create_openai_client():\n    client = OpenAI(\n        api_key=os.environ.get(\"OPENAI_API_KEY\"),\n        http_client=CustomHTTPClient()\n    )\n\n    assert isinstance(client, OpenAI)\n    assert isinstance(client._client, CustomHTTPClient)\n    assert isinstance(client.chat._client, OpenAI)\n    return client\n\nopenai_client = create_openai_client()\n\n```\n\n### Объяснение кода\n\nКласс `CustomHTTPClient` наследует класс `httpx.Client` библиотеки `httpx` и переопределяет метод `send`. В нем выполняется отправка запроса и обработка возможных исключений. Также в этом методе применяется декоратор `backoff.on_exception` для автоматической повторной отправки запроса при возникновении определенных исключений.\n\nФункция `create_openai_client()` создает экземпляр класса `OpenAI` используя API-ключ, хранящийся в переменной окружения `OPENAI_API_KEY`, и разработанный выше кастомный HTTP-клиент. Этот экземпляр используется для общения с OpenAI API. Проверка типов на подтверждение корректности созданных объектов выполняется с помощью `assert`.\n\nВ итоге, создается экземпляр OpenAI клиента и сохраняется в глобальной переменной `openai_client` для дальнейшего использования.",
    "title": "\"Создание HTTP-клиента для взаимодействия с OpenAI API\"",
    "summary": "Этот Markdown файл описывает процесс создания HTTP-клиента с использованием библиотеки OpenAI для взаимодействия с OpenAI API. Внутри приведён пример кода с подробным объяснением работы методов и классов, включая обработку исключений и последующую реализацию HTTP-клиента."
  },
  "../blog/02_snippets/auto_generated/python/update_index_with_summaries.md": {
    "filename": "update_index_with_summaries.md",
    "path": "../blog/02_snippets/auto_generated/python/update_index_with_summaries.md",
    "relative_path": "02_snippets/auto_generated/python/update_index_with_summaries.md",
    "hash": "2e1464d564ad1a327a694eb9d2c6332a",
    "is_updated": false,
    "content": "# Создание индексной страницы блога с использованием Python и Open AI\n\nСледующий сниппет на Python воспроизводит процесс чтения файлов статей блога, обращение к GPT-4 для генерации заголовка и краткого описания для каждого файла, а затем создания индексной страницы, содержащей ссылки на все статьи со сгенерированными заголовками и описаниями.\n\n```python\nimport hashlib\nimport json\nimport os\nimport sys\n\nfrom jinja2 import Environment, FileSystemLoader\nfrom tqdm import tqdm\n\nfrom clients.openai import openai_client\n```\n\nВначале импортируются необходимые для работы библиотеки. `hashlib` и `json` используются для работы с хэш-функциями и файлами JSON соответственно, `os` и `sys` – для работы с операционной системой, `jinja2` отвечает за работу с HTML-шаблонами, а `tqdm` позволяет отображать прогресс выполнения операций. `openai_client` это пользовательский класс, который предоставляет удобный интерфейс для взаимодействия с API OpenAI.\n\n```python\nfrom clients.openai import openai_client\n\n\ndef generate_title(file_path):\n    ...\n    response = openai_client.chat.completions.create(\n        ...\n        model=\"gpt-4\",\n    )\n    return response.choices[0].message.content\n```\nЗдесь функция `generate_title` использует GPT-4 для создания краткого заголовка файла Markdown. Входыми данными здесь является путь до файлика.\n\n```python\ndef generate_summary(file_path):\n    ...\n    response = openai_client.chat.completions.create(\n        ...\n        model=\"gpt-4\",\n    )\n    return response.choices[0].message.content\n```\n\nФункция `generate_summary` применяет модель генерации текста GPT-4 для создания краткого описания для файла Markdown.\n\n```python\ndef hash_content(content):\n    return hashlib.md5(content.encode()).hexdigest()\n```\n\nФункция `hash_content` использует md5 для создания хэш-значения содержимого файла. Это используется впоследствии для определения, был ли файл изменен или нет.\n\n```python\ndef update_index(directory, index_path):\n    ...\n```\n\nФункция `update_index` создает или обновляет индексный файл JSON для всех файлов Markdown в указанной директории. В этом JSON-файле для каждого файла Markdown хранится информация, требующаяся для генерации индексной страницы (путь к файлу, заголовок, описание и т.д.).\n\n```python\n@pysnooper.snoop()\ndef generate_index_md(json_path, output_md_path):\n    ...\n```\n\nФункция `generate_index_md` берет данные из JSON-файла, который был сгенерирован или обновлен функцией `update_index`, и использует шаблонизатор Jinja2 для создания нового или обновления существующего Markdown-файла для главной страницы блога. Этот Markdown-файл содержит ссылки на все статьи блога со сгенерированными заголовками и описаниями.\n\n```python\nif __name__ == \"__main__\":\n    ...\n    try:\n        update_index(blog_directory, index_data_path)\n    except Exception as e:\n        print(f\"update_index failed with error: {e}\")\n        sys.exit(1)\n\n    generate_index_md(index_data_path, index_file_path)\n```\n\nЭта часть кода выполняется, когда script запускается из командной строки. Сначала выполняется функция `update_index`, а затем `generate_index_md`.",
    "title": "\"Генерация индекса блога с Python и OpenAI\"",
    "summary": "Этот Markdown-файл представляет собой руководство по созданию индексной страницы блога с использованием Python и Open AI. В статье описывается процесс чтения файлов статей блога, генерации заголовка и краткого описания для каждого файла с помощью GPT-4, и последующего создания индексной страницы со ссылками на все статьи."
  },
  "../blog/02_snippets/auto_generated/python/snippet_generator.md": {
    "filename": "snippet_generator.md",
    "path": "../blog/02_snippets/auto_generated/python/snippet_generator.md",
    "relative_path": "02_snippets/auto_generated/python/snippet_generator.md",
    "hash": "43e0c1be6b2ad924d9ba14efa82b8f16",
    "is_updated": false,
    "content": "# Объяснение кода из файла src/snippet_generator.py\n\nДанный скрипт на Python предназначен для автоматического создания кратких представлений (англ. snippets) кода в формате Markdown с помощью модели OpenAI gpt-4 для генерации естественного языка.\n\n```python\nimport sys\nfrom clients.openai import openai_client\n```\n\nВ начале мы импортируем необходимые модули Python. Модуль `sys` необходим для работы со скриптом из командной строки, а модуль `openai_client` – это наш клиентский класс для работы с OpenAI.\n\n```python\ndef generate_snippet(file_path, output_path):\n    with open(file_path, 'r') as file:\n        code = file.read()\n```\nФункция `generate_snippet` принимает два аргумента: путь к файлу с исходным кодом и путь для вывода файла markdown. Внутри файла производится чтение исходного кода.\n\n```python\n    if not code:\n        raise ValueError(f\"File {file_path} is empty\")\n```\nЗдесь мы проверяем, что файл с исходным кодом не пуст и, если это не так, добавляем сообщение об ошибке.\n\n```python\n    response = openai_client.chat.completions.create(\n        ...\n    model=\"gpt-4\",\n    )\n```\n\nЗатем мы используем модель OpenAI gpt-4 для создания руководства о том, что делает наш код. В данном примере подразумевается, что код должен быть объяснен на русском языке и быть оформлен в стиле мини статьи для блога.\n\n```python\n    snippet = response.choices[0].message.content\n    markdown_snippet = f\"{snippet}\"\n```\n\nИз полученного ответа мы берём сгенерированный сниппет и сохраняем его в переменную `markdown_snippet`.\n\n```python\n    with open(output_path, 'w') as output_file:\n        output_file.write(markdown_snippet)\n```\n\nВ конце функции `generate_snippet` мы сохраняем наш сниппет в файл по указанному пути.\n\n```python\nif __name__ == \"__main__\":\n    ...\n    try:\n        generate_snippet(source_file_path, output_markdown_path)\n    except Exception as e:\n        print(f\"generate_snippet failed for {source_file_path} with error: {e}\")\n        sys.exit(1)\n```\n\nВ основной части скрипта мы запрашиваем аргументы из командной строки и запускаем функцию генерации сниппета. Если что-то пойдет не так, мы выводим сообщение об ошибке и закрываем скрипт с ненулевым кодом возврата.\n",
    "title": "\"Автоматическая генерация сниппетов кода Python\"",
    "summary": "Файл 'snippet_generator.md' содержит подробное объяснение работы Python скрипта, предназначенного для автоматического создания кратких представлений (snippets) кода в формате Markdown с использованием модели OpenAI gpt-4. В статье детально рассмотрены основные части кода, их функции и принцип работы."
  },
  "../blog/02_snippets/auto_generated/python/text_cleaner.md": {
    "filename": "text_cleaner.md",
    "path": "../blog/02_snippets/auto_generated/python/text_cleaner.md",
    "relative_path": "02_snippets/auto_generated/python/text_cleaner.md",
    "hash": "1581098e21940c2140e442f3df231cb9",
    "is_updated": false,
    "content": "# Приведение текстовых данных к нужному виду с помощью Python\n\nВ работе с текстовыми данными часто приходится сталкиваться с необходимостью их очистки и препроцессинга. Представленный в этом сниппете код на языке Python позволяет это выполнить. Давайте взглянем на основные функции и объясним, что они делают.\n\n### Основные идеи при очистке текста\n\n- Удаление лишних символов\n- Замена повторяющихся единиц текста\n- Объединение слов, разделенных дефисами\n- Удаление \"мусорного\" текста\n\n### Процедуры очистки\n\nВо-первых, мы импортируем две библиотеки: `re` – для работы с регулярными выражениями и `regex`, который даёт больше возможностей по работе с Юникод символами.\n\n```python\nimport re\nimport regex\n```\n\nЗдесь у нас определен регулярное выражение, которое исключает все, что не входит в \"белый список\". Этот список включает в себя кириллицу, латиницу, некоторые спецсимволы и цифры. \n\n```python\nNOT_IN_WHITELIST_REGEX = r\"[^АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯяIi1\\-\\.\\,\\:\\; \\\n-!?–…«»1234567890)(№*×><]+\"\n```\n\nСледующие функции это набор различных очисток:\n\n- Удаление текстовых фрагментов с кодировками наборов символов `(cid:...)`\n- Удаление указаний секций `(i:...)`\n- Удаление слов, написанных заглавными буквами\n- Замена множественных новых строк на одну\n- Удаление пробелов перед знаками пунктуаций\n- Удаление слов с `II`\n- Добавление пробела после запятой\n- Удаление наборов больше одного пробела\n- Удаление лишних точек\n\nПример:\n\n```python\ndef remove_uppercase_words(text):\n    return re.sub(r\"\\b[А-ЯЁA-Z]{5,}\\b\", \"\", text)\n```\n\nЭта функция принимает на вход строку текста, затем использует функцию `sub` из библиотеки `re` для замены регулярному выражению (`\\b[А-ЯЁA-Z]{5,}\\b`, что является любым словом из 5 или более заглавных букв) на пустую строку.\n\nВ конце есть функция `clean_text`, которая объединяет весь процесс очистки. Она вызывает каждую функцию очистки по очереди на входном тексте.\n\n```python\ndef clean_text(text):\n    if not text:\n        return \"\"\n\n    clean_functions = [\n        remove_non_whitelisted_chars,\n        remove_html_tags,\n        ...\n        unify_hyphenated_words,\n    ]\n\n    for func in clean_functions:\n        text = func(text)\n\n    return text\n```\n\nВ остальном, в тексте удаляются HTML теги, эмодзи, повторяющаяся пунктуация и \"мусорный\" текст. \n\nТаким образом, данный код эффективно и быстро обработает большие тексты, оставляя только значимую информацию.",
    "title": "\"Очистка текста в Python: подробное руководство\"",
    "summary": "Markdown файл '../blog/02_snippets/auto_generated/python/text_cleaner.md' содержит руководство по очистке и предобработке текстовых данных на языке Python. Описываются методы удаления лишних символов, повторяющихся единиц текста и \"мусорного\" текста, замены слов, объединение слов, разделённых дефисами. Подробно разбираются функции препроцессинга и демонстрируется пример их работы."
  },
  "../blog/02_snippets/auto_generated/notebooks/02_00_text_analyze_bigram_extract.md": {
    "filename": "02_00_text_analyze_bigram_extract.md",
    "path": "../blog/02_snippets/auto_generated/notebooks/02_00_text_analyze_bigram_extract.md",
    "relative_path": "02_snippets/auto_generated/notebooks/02_00_text_analyze_bigram_extract.md",
    "hash": "0a44edcdb5c24239e20a4aa65653269c",
    "is_updated": false,
    "content": "# Анализ текста и извлечение биграмм с помощью Python и NLTK\n\nДанный код позволяет исследовать связь между словами (более конкретно биграммами) в заданном текстовом файле. \nБиграмма – это пара последовательных слов в тексте.\n\n## Чтение данных\nМы начинаем со стандартного импорта библиотек. Путь до файла указывается в переменной `file_path`. `BUF_SIZE` параметр, который контролирует сколько символов мы читаем из файла за одну итерацию. \n```python\nimport os\nfrom collections import Counter\nimport nltk\n\nfile_dir = '../data/processed'\nfile_name = 'oshhamaho.txt'\nfile_path = os.path.join(file_dir, file_name)\nBUF_SIZE = 100000\n```\n\n## Извлечение биграмм\n```python\ncnt = Counter()\nwith open(file_path) as f:\n    tmp_raw = f.read(BUF_SIZE)\n    while tmp_raw:\n        tokens = nltk.word_tokenize(tmp_raw)\n        text = nltk.Text(tokens)\n        clc = text.collocation_list(5000)\n        cnt.update(clc)\n        tmp_raw = f.read(BUF_SIZE)\n```\n`nltk.word_tokenize` разбивает текст на отдельные слова. `nltk.Text` представляет текст как список слов. Метод `collocation_list` выдает наиболее частые биграмы текста. Результат сохраняется в `cnt`, который является объектом счетчика слов.\n\n## Создание DataFrame\n```python\nimport pandas as pd\n\ndata = [\n    {\n        'w_bigram': ' '.join(bigram),\n        'freq': freq\n    }\n    for bigram, freq in cnt.items()\n    if freq > 2\n]\n\ndf = pd.DataFrame(data)\ndf = df.sort_values('freq', ascending=False)\n```\nЭти данные затем преобразуются в DataFrame для удобства просмотра и дальнейшей обработки.\n\n## Сохранение данных\n```python\ndf.to_csv(\n    os.path.join('../data/processed/word_bigrams', file_name.replace('.txt', '.csv')),\n    index=False,\n)\n```\nDataFrame сохраняется в CSV файл.\n\n## Проверка вероятности последовательности биграмм\n```python\nfrom nltk.probability import ConditionalFreqDist, ConditionalProbDist, MLEProbDist\n\ncfdist = ConditionalFreqDist()\n\nfor bigram, freq in cnt.items():\n    word1, word2 = bigram\n    cfdist[word1][word2] = freq\n\ncpdist = ConditionalProbDist(cfdist, MLEProbDist)\n```\nЗдесь мы проверяем, насколько вероятно, что одно слово следует за другим.\n\n## Получение вероятности для каждого слова\n```python\ndef get_prob(word1):\n    prob_dict = {}\n    for word_2 in cpdist[word1].samples():\n        prob_dict[word_2] = cpdist[word1].prob(word_2) * 100\n    return prob_dict\n\nsorted(get_prob('псалъэ').items(), key=lambda x: x[1], reverse=True)\n```\n`get_prob` возвращает словарь, показывающий, какие слова наиболее вероятно идут после данного слова. \n\nТакими способами мы можем проводить простой и исчерпывающий анализ текста. Код довольно прост, но позволяет получить полезную информацию о структуре текста и взаимосвязи слов в этом тексте.",
    "title": "# Извлечение и анализ биграмм из текста",
    "summary": "Этот Markdown файл представляет собой инструкцию по анализу текста и извлечению биграмм с использованием Python и библиотеки NLTK. Объясняется процесс чтения данных из файла, извлечения биграмм, создания DataFrame для удобства обработки, сохранения полученных данных и расчета вероятности специфической последовательности слов в тексте."
  },
  "../blog/02_snippets/auto_generated/notebooks/02_01_text_analyze_word_frequent_compare_by_source.md": {
    "filename": "02_01_text_analyze_word_frequent_compare_by_source.md",
    "path": "../blog/02_snippets/auto_generated/notebooks/02_01_text_analyze_word_frequent_compare_by_source.md",
    "relative_path": "02_snippets/auto_generated/notebooks/02_01_text_analyze_word_frequent_compare_by_source.md",
    "hash": "dc7d999ff093e9cf47307dfd99780986",
    "is_updated": false,
    "content": "# Анализ частоты слов сравнение по источникам\n\nЭтот сниппет иллюстрирует процесс анализа частоты слов в текстах из разных источников. Анализ проводится на примере трех отдельных датасетов, которые содержат информацию о частоте слов.\n\n## Загрузка данных\nИсходные датасеты загружаются при помощи функции pandas read_csv.\n\n```python\nimport os\nimport pandas as pd\n\ninput_dir = '../data/processed/word_freqs'\n\ndf_freq_apkbr_ru = pd.read_csv(os.path.join(input_dir, 'freq_1000000_apkbr_ru.csv'))\ndf_freq_elgkbr_ru = pd.read_csv(os.path.join(input_dir, 'freq_1000000_elgkbr_ru.csv'))\ndf_freq_oshhamaho = pd.read_csv(os.path.join(input_dir, 'freq_1000000_oshhamaho.csv'))\n```\n## Подсчет топа слов\nВ следующем блоке кода вычисляются топ-100 самых часто встречающихся слов для каждого из датасетов.\n\n```python\ntop_count = 100\ndf_freq_apkbr_ru_top = df_freq_apkbr_ru.sort_values(by='freq', ascending=False).head(top_count)\ndf_freq_elgkbr_ru_top = df_freq_elgkbr_ru.sort_values(by='freq', ascending=False).head(top_count)\ndf_freq_oshhamaho_top = df_freq_oshhamaho.sort_values(by='freq', ascending=False).head(top_count)\n```\n## Подсчет уникальных слов\nДалее для каждого источника вычисляются уникальные слова, которые встречаются только в том определенном источнике и отсутствуют в двух других.\n\n```python\napkbr_ru = df_freq_apkbr_ru_top.merge(\n    df_freq_elgkbr_ru_top, \n    how='left', \n    left_on='word', \n    right_on='word', \n    suffixes=('_apkbr_ru', '_elgkbr_ru')\n).merge(\n    df_freq_oshhamaho_top, \n    how='left', \n    left_on='word', \n    right_on='word', \n)\napkbr_ru.rename(columns={'freq': 'freq_oshhamaho'}, inplace=True)\napkbr_ru_uniq = apkbr_ru[\n    (apkbr_ru.freq_elgkbr_ru.isna()) & \n    (apkbr_ru.freq_oshhamaho.isna())\n]\n```\nАналогично для остальных источников.\n\n## Заключение\n\nВ результате построена таблица топ-100 уникальных слов для каждого источника. На основе этих данных можно делать выводы о характерных особенностях каждого источника. В частности, это может быть полезно при анализе тематической спецификации текстов.",
    "title": "# Сравнительный анализ частотности слов по источникам",
    "summary": "'Анализ частоты слов сравнение по источникам' - файл, иллюстрирующий анализ частотности слов в текстах из различных источников. Описывает процесс загрузки и предобработки данных, расчёт топа самых часто встречающихся слов и уникальных слов, с особым акцентом на определение характерных особенностей каждого источника."
  },
  "../blog/02_snippets/auto_generated/notebooks/02_01_text_analyze_word_extract.md": {
    "filename": "02_01_text_analyze_word_extract.md",
    "path": "../blog/02_snippets/auto_generated/notebooks/02_01_text_analyze_word_extract.md",
    "relative_path": "02_snippets/auto_generated/notebooks/02_01_text_analyze_word_extract.md",
    "hash": "3ffba9f5f3c693d8805b9b5f457d0750",
    "is_updated": false,
    "content": "# Анализ текста и извлечение слов с Python \n\nВ данном сниппете используется язык программирования Python для создания частотного словаря слов на основе выбранных текстовых файлов. Иными словами, мы просмотрим выбранные текстовые файлы, распарсим их на отдельные слова, посчитаем частотность каждого слова и записим результаты в новый файл. \n\nБиблиотеки, которые нам понадобятся:\n- `collections` - для использования класса `Counter`, который упростит подсчет частоты появления слов.\n- `nltk` - для разбиения текста на слова (токенов).\n- `pandas` - для создания и манипуляции с данными в формате таблиц.\n- `os` - для работы с файловой системой.\n\n### Объяснение кода\n\n**1. Импорт библиотек и установка параметров**\n\nВ начале импортируются все необходимые библиотеки. Включается кнопка скачивания стоп-слов для nltk. Устанавливаются пути до директорий с входными и выходными данными. Создаётся счетчик слов, который будет общим для всех файлов.\n\n```python\nfrom collections import Counter\nimport os\nimport nltk\nimport pandas as pd\n\nnltk.download('stopwords')\n\ninput_dir = '../data/processed'\noutput_dir = '../data/processed/word_freqs'\n\nos.makedirs(output_dir, exist_ok=True)\n\nBUF_SIZE = 100000\nword_limit = 1000000\n\nall_word_counter = Counter()\n```\n**2. Обработка файлов**\n\nЗатем идет описание двух функций, которые обрабатывают счетчики слов - `save_freqs_flat` и `save_with_freq`. Первая функция просто сохраняет слова отсортированными по частоте встречаемости, вторая сохраняет слова вместе с частотой, при этом можно настроить ограничение на минимальную частоту, которая нужна для сохранения в файл.\n\n```python\ndef save_freqs_flat(counter, output_path):\n    # ...\n\ndef save_with_freq(counter, output_path, freq_limit=0):\n    # ...\n```\n\nЗатем идёт цикл обработки входных файлов. Для каждого файла открывается поток чтения, данные читаются блоками заданного размера, токенизируются с помощью nltk и эти слова передаются в соответствующие счётчики - общий и отдельный для каждого файла. Результаты сохраняются в два файла для каждого исходного.\n\n```python\nfor input_file_name in ['oshhamaho.txt', 'apkbr_ru.txt', 'elgkbr_ru.txt']:\n    # ...\n```\n\nЭтот код - простой, но мощный инструмент для начального анализа текста, который может помочь в поиске ключевых слов и выявлении общих тем в текстовых данных.",
    "title": "# Создание частотного словаря слов с Python",
    "summary": "Этот Markdown файл представляет собой руководство по анализу текста и извлечению ключевых слов с использованием Python. В нем объясняется, как создать частотный словарь слов из выбранных текстовых файлов с помощью библиотек `collections`, `nltk`, `pandas` и `os`."
  },
  "../blog/02_snippets/auto_generated/notebooks/00_clean_scraped_text.md": {
    "filename": "00_clean_scraped_text.md",
    "path": "../blog/02_snippets/auto_generated/notebooks/00_clean_scraped_text.md",
    "relative_path": "02_snippets/auto_generated/notebooks/00_clean_scraped_text.md",
    "hash": "8d73c66c44a69974341e30a29d9afa86",
    "is_updated": false,
    "content": "# Очистка и предобработка текста\n\nВ этом модуле Python мы будем использовать numpy, pandas и os для считывания, очистки и сохранения данных, полученных в результате web-скрейпинга.\n\n```python\nimport os  # библиотека для работы с файлами и папками\nimport pandas as pd  # библиотека для работы с таблицами данных\nfrom src.text_cleaner import clean_text  # функция для очистки текста\n```\n\nЭтот код используется для импорта необходимых библиотек и функций.\n\n```python\nexternal_dir = '../data/external/'\ninterim_dir = '../data/interim/'\nos.makedirs(interim_dir, exist_ok=True)\n\nprocessed_dir = '../data/processed/'\nos.makedirs(processed_dir, exist_ok=True)\n```\n\nМы создаем папку для сохранения данных после очистки.\n\n```python\nfor file_name in ['apkbr_ru.jsonl', 'elgkbr_ru.jsonl']:    \n    df = pd.read_json(os.path.join(external_dir, file_name), lines=True)\n    df['content'] = df['content'].apply(clean_text)\n```\n\nЭтот код обрабатывает файлы json. Он загружает каждый файл в DataFrame (df), затем применяет функцию clean_text к каждой строке в колонке 'content'.\n\n```python\ndf.to_json(\n        os.path.join(interim_dir, file_name), \n        orient='records', \n        lines=True,\n        force_ascii=False\n    )\n```\n\nПосле очистки данные сохраняются обратно в json-файл.\n\n```python\ntext = '\\n'.join(df['content'].tolist())\n    with open(os.path.join(processed_dir, file_name.replace('.jsonl', '.txt')), 'w') as f:\n        f.write(text)\n```\n\nЗдесь содержимое каждой строки объединяется в одну большую строку, которая затем сохраняется в файл .txt. \n\nИтак, этот сниппет берет необработанные данные, полученные из веб-скрапинга, и чистит их от нестандартных символов и пробелов, чтобы подготовить их к дальнейшей обработке. После очистки данные сохраняются в двух форматах - .json и .txt.",
    "title": "\"Очистка и сохранение веб-скрейпинг данных\"",
    "summary": "Файл '00_clean_scraped_text.md' - это подробное руководство по очистке и предобработке текстовых данных, полученных с помощью web-скрапинга. С использованием различных библиотек Python и созданных функций, автор обрабатывает json-файлы, считывает и очищает текст от нежелательных символов, затем сохраняет обработанные данные в форматы .json и .txt для дальнейшего анализа."
  },
  "../blog/02_snippets/auto_generated/notebooks/02_02_text_analyze_tokenizer_bpe.md": {
    "filename": "02_02_text_analyze_tokenizer_bpe.md",
    "path": "../blog/02_snippets/auto_generated/notebooks/02_02_text_analyze_tokenizer_bpe.md",
    "relative_path": "02_snippets/auto_generated/notebooks/02_02_text_analyze_tokenizer_bpe.md",
    "hash": "82e6e5f56dd863976287998068db0839",
    "is_updated": false,
    "content": "# Сниппет: Создание токенизатора с помощью BPE\nВ этом сниппете мы будем создавать токенизатор, использующий подход Byte Pair Encoding (BPE) для обучения на конкретном текстовом файле. \n\n## Что такое BPE?\nBPE - это метод, который позволяет сжимать текст без потери информации. Обучение на BPE позволяет токенизатору делить слова на подслова, если они редкие или вообще не встречались в учебном материале.\n\n```python\nimport os\n\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\n\ntokenizer = Tokenizer(BPE())\n```\n\nСначала мы импортируем необходимые модули и создаем токенизатор BPE.\n\n```python\ncustom_tokens = [\n    # кастомные токены\n]\n\ntokenizer.add_tokens(custom_tokens)\n```\n\nМы можем добавить кастомные токены, которые не должны быть разделены на подслова.\n\n```python\nvocab_size = 100000\n\ntrainer = BpeTrainer(\n    vocab_size=vocab_size,\n    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n    show_progress=True,\n)\n```\n\nСоздаем тренер BPE, указываем размер словаря и специальные токены. \n\n```python\nfile_path = os.path.join(file_dir, file_name)\n\ntokenizer.train([file_path], trainer)\n```\n\nТренируем токенизатор на нашем файле.\n\n```python\ntokenizer_dir = '../data/processed/tokenizer'\ntokenizer_file_name = f'bpe_{vocab_size}.tokenizer.json'\ntokenizer_file_path = os.path.join(tokenizer_dir, tokenizer_file_name)\n\ntokenizer.save(tokenizer_file_path, pretty=True)\n```\n\nСохраняем обученный токенизатор в файл.\n\n```python\nencoded = tokenizer.encode('Текст для токенизации.')\nprint(encoded.tokens)\n```\n\nИспользуем токенизатор для преобразования текста в последовательность токенов.\n\nПрименяя процедуру описанную выше, вы можете создавать настраиваемые токенизаторы, которые будут эффективно работать с вашими конкретными текстовыми данными.",
    "title": "\"Создание токенизатора с использованием BPE\"",
    "summary": "Статья описывает процесс создания и обучения собственного токенизатора на основе подхода Byte Pair Encoding (BPE). Приводится код и пошаговое пояснение, от обучения до его использования и сохранения. Эта информация поможет в создании настроенных токенизаторов для работы с специфическими текстовыми данными."
  },
  "../blog/02_snippets/auto_generated/notebooks/02_03_text_analyze_tokenizer_unigram.md": {
    "filename": "02_03_text_analyze_tokenizer_unigram.md",
    "path": "../blog/02_snippets/auto_generated/notebooks/02_03_text_analyze_tokenizer_unigram.md",
    "relative_path": "02_snippets/auto_generated/notebooks/02_03_text_analyze_tokenizer_unigram.md",
    "hash": "1ea814f9670b17a16f4b2292f7082baf",
    "is_updated": false,
    "content": "# Объяснение кода для токенизации текста методом unigram\nДанный код производит токенизацию текстового файла (разбиение текста на отдельные элементы - токены). В качестве модели для разбиения используется метод unigram.\n\n\n```python\nimport os\n\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import Unigram\nfrom tokenizers.trainers import UnigramTrainer\n\ntokenizer = Tokenizer(Unigram())\n```\nПервым делом мы импортируем нужные нам библиотеки и создаем экземпляр Unigram токенизатора.\n\n```python\n...\ncustom_tokens = [ ... ]\n\ntokenizer.add_tokens(custom_tokens)\n\nvocab_size = 30000\n\ntrainer = UnigramTrainer(\n    vocab_size=vocab_size,\n    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n    show_progress=True,\n    unk_token='UNK'\n)\n```\nПеред обучением токенизатора мы добавляем кастомные токены и создаем экземпляр тренера для настройки параметров обучения и обучения токенизатора.\n\n```python\nfile_dir = '../data/processed/word_freqs'\nfile_name = 'freq_1000000_oshhamaho.txt'\nfile_path = os.path.join(file_dir, file_name)\n\ntokenizer.train([file_path], trainer)\n```\nВ этом блоке мы указываем путь к файлу, который будем использовать для обучения токенизатора, и запускаем процесс обучения.\n\n```python\ntokenizer_dir = '../data/processed/tokenizer'\ntokenizer_file_name = f'words_unigram_{vocab_size}.tokenizer.json'\ntokenizer_file_path = os.path.join(tokenizer_dir, tokenizer_file_name)\ntokenizer.save(tokenizer_file_path, pretty=True)\n```\nПосле обучения токенизатора мы сохраняем его в файл, чтобы иметь возможность использовать его позже без необходимости повторного обучения.\n\n```python\nencoded = tokenizer.encode('ЦӀыхум и гум удыхьэн папщӀэ, абы и бгъэр зэгуэбгъэжын хуейкъым.')\nprint(encoded.tokens)\n```\nЭтот код проверяет работоспособность токенизатора. Мы берем пример текста и смотрим, как он будет разбит на токены.\n\n```python\nfor word in test_words:\n    print(tokenizer.encode(word).tokens)\n```\nЭтот код проверяет работоспособность токенизатора на серии тестовых слов.\n\nТаким образом, мы создали Unigram токенизатор, обучили его и проверили его работу на примерах.",
    "title": "# Токенизация текста с помощью метода Unigram\n",
    "summary": "Статья посвящена процессу создания, обучения и проверки работы Unigram токенизатора для разбиения текстовых файлов на отдельные элементы - токены. В статье подробно разбирается использование нужных библиотек, добавление кастомных токенов, настройка параметров обучения и сохранение готового токенизатора в файл."
  },
  "../blog/02_snippets/auto_generated/notebooks/01_extract_text_from_pdf.md": {
    "filename": "01_extract_text_from_pdf.md",
    "path": "../blog/02_snippets/auto_generated/notebooks/01_extract_text_from_pdf.md",
    "relative_path": "02_snippets/auto_generated/notebooks/01_extract_text_from_pdf.md",
    "hash": "92c822ffa214434c972bb610645283f3",
    "is_updated": false,
    "content": "## Извлечение текста из PDF файла с помощью Python\n\nPython обладает множеством библиотек, с помощью которых можно обрабатывать различные типы файлов. В данном сниппете мы рассмотрим как извлечь текст из PDF файлов и сохранить его в txt файлы с помощью модулей `textract`, `os` и `tqdm`.\n\nПрежде всего, с помощью модуля `os` создаётся папка для сохранения текстовых файлов.\n\n```python\noshhamaho_external_pdf_dir = '../data/external/oshhamaho'\noshhamaho_interim_txt_dir = '../data/interim/oshhamaho'\nos.makedirs(oshhamaho_interim_txt_dir, exist_ok=True)\n\npdf_files = [f for f in os.listdir(oshhamaho_external_pdf_dir) if f.endswith('.pdf')]\n```\n\nЗатем определена функция `extract_text_from_pdf_file()`, которая берёт на вход путь к PDF файлу, извлекает из него текст и возвращает этот текст в виде строки, с использованием библиотеки `textract`.\n\n```python\nimport textract\ndef extract_text_from_pdf_file(pdf_path):\n    try:\n        text = textract.process(pdf_path, method='pdfminer')\n    except Exception:\n        return ''\n    text = text.decode('utf-8')\n    return text\n```\n\nЦиклом пробегаем по всем файлам с расширением '.pdf', извлекаем текст и сохраняем как текстовый файл '.txt'.\n\n```python\npgbar = tqdm(pdf_files)\n\nfor pdf_file in pgbar:\n    pgbar.set_description(pdf_file)\n    pdf_path = os.path.join(oshhamaho_external_pdf_dir, pdf_file)\n    \n    text_i = extract_text_from_pdf_file(pdf_path)\n    cleaned_text = clean_text(text_i) \n\n    text_file = pdf_file.replace('.pdf', '.txt')\n    text_path = os.path.join(oshhamaho_interim_txt_dir, text_file)\n    \n    with open(text_path, 'w') as f:\n        f.write(cleaned_text)\n```\n\nВ данном коде используется `tqdm` для создания интерактивной строки прогресса. Это не обязательно, но увеличивает удобство работы.\n\nЗатем из каждого текстового файла текст собирается, очищается и склеивается в одну большую строку, после чего записывается в новый файл.\n\n```python\ntxt_files = sorted([f for f in os.listdir(oshhamaho_interim_txt_dir) if f.endswith('.txt')])\nprocessed_dir = '../data/processed/'\nos.makedirs(processed_dir, exist_ok=True)\n\noshhamaho_all_txt_path = os.path.join(processed_dir, 'oshhamaho.txt')\n\npgbar = tqdm(txt_files)\ntext = ''\n\nfor txt_file in pgbar:\n    pgbar.set_description(txt_file)\n    txt_path = os.path.join(oshhamaho_interim_txt_dir, txt_file)\n    \n    with open(txt_path, 'r') as f:\n        text_i = f.read()\n    \n    cleaned_text = clean_text(text_i) \n    text += cleaned_text + '\\n'\n\nwith open(oshhamaho_all_txt_path, 'w') as f:\n    f.write(text)\n```\nЭтот код можно использовать для автоматизации процесса извлечения текста из большого количества PDF файлов. Он легко модифицируется в соответствии с требованиями вашего проекта.",
    "title": "\"Извлечение текста из PDF файлов с использованием Python\"",
    "summary": "Статья описывает процесс извлечения текста из PDF файлов с использованием Python и модулей `os`, `tqdm` и `textract`. Описываются шаги создания репозитория для хранения текстовых файлов, определение функции для извлечения текста, применение функции для обработки всех PDF-файлов в репозитории и последующая очистка текста."
  },
  "../blog/01_articles/01_kabardian_language_data_collection.md": {
    "filename": "01_kabardian_language_data_collection.md",
    "path": "../blog/01_articles/01_kabardian_language_data_collection.md",
    "relative_path": "01_articles/01_kabardian_language_data_collection.md",
    "hash": "3b0ee867a52f8b19c8374aa32cad14d9",
    "is_updated": false,
    "content": "# Кабардинский язык в Цифровом Мире: Этап Сбора Данных \n\n### **Введение**\n\n**В эпоху цифровизации, когда мир становится все более глобализированным, многие малоресурсные языки, включая кабардинский, сталкиваются с угрозой исчезновения. Основная проблема этих языков – не уменьшение числа носителей, а недостаток цифровой поддержки и ограниченное присутствие в интернете, что в будущем может оказать серьезное влияние на их выживание. Кабардинский язык, богатый своей историей и культурой, заслуживает особого внимания в свете сохранения культурного наследия.**\n\n**Цель этой статьи - исследовать процесс сбора текстов на кабардинском языке из цифровых источников, таких как новостные сайты. Эти источники представляют собой ценный ресурс для анализа современного использования языка. В дальнейшем планируется изучение других методов сбора данных, включая материалы из социальных сетей и тексты книг.**\n\n**Выбор кабардинского языка для данного исследования обусловлен его значением как моего родного языка и стремлением сохранить его уникальное культурное наследие. Эта статья представляет собой шаг к цифровому возрождению кабардинского языка и привлечению внимания специалистов в области технологий к этому и другим малоресурсным языкам.**\n\n\n### **Выбор Источников Данных**\n\nВыбор источников данных для сбора текстов на кабардинском языке был направлен на эффективность и максимальное покрытие. Основным критерием стало наличие обширного и доступного контента, что привело к выбору крупных новостных сайтов. Эти сайты предоставляют богатый массив текстов, что идеально подходит для начального этапа сбора данных.\n\nВ качестве основных источников были выбраны следующие ресурсы:\n\n- **Электронная газета \"Адыгэ Псалъэ\"**: Один из ведущих источников, предоставляющий обширный массив новостей и статей на кабардинском языке.\n- **Электронная газета \"Кабардино-Балкария\" (статьи на кабардинском языке)**: Ещё один ключевой ресурс, обогащающий нашу базу данных разнообразными текстами.\n- **Журнал Iуащхьэмахуэ (Эльбрус)**: Журнал, который вносит вклад в разнообразие собираемого материала.\n\nЭти источники были выбраны не только за их доступность, но и за способность предоставить разнообразный набор текстов, отражающих современное использование кабардинского языка. Такой подход позволяет собрать широкий спектр материалов - от новостных статей до культурных и образовательных публикаций.\n\n### **Использование Scrapy для Сбора Данных**\n\n#### Почему Scrapy?\n\nЕсть много достойных инструментов для веб-скрапинга [awesome-web-scraping](https://github.com/lorien/awesome-web-scraping?tab=readme-ov-file).\nДа и наверное самым простым и быстрым способом собрать данные с веб-сайта является использование библиотеки `requests` + `BeautifulSoup` в Python.\nНо для долгосрочных проектов веб-скрапинга, лучше использовать более мощные инструменты, которые обеспечивают эффективность и надежность сбора данных.\n\nДля этого проекта я выбрал Scrapy, исходя из его способности эффективно масштабироваться и адаптироваться к растущим потребностям сбора данных. \nScrapy выделяется своим богатым функционалом и предоставляет гибкие возможности для парсинга и обработки данных. \nНапример встроенные механизмы кеширования запросов и ответов, настройкой экспорта данных в различные форматы, а также управлением интенсивностью обхода веб-сайтов без написания дополнительного кода.\nК тому же Scrapy с его удобным API для расширения, не ограничивает возможности тонкой настройки и расширения функционала.\nЭто делает его хорошим выбором для долгосрочных проектов веб-скрапинга.\n\n\n#### Основные Шаги Настройки\n\n1. **Кеширование Запросов и Ответов**: Одной из важных настроек в Scrapy является механизм кеширования запросов и ответов. \nЭто особенно полезно при работе с новостными сайтами, где контент обычно статичен и не обновляется часто. \nКеширование позволяет избежать повторных запросов при последующих обходах, снижая нагрузку на сайты и ускоряя процесс продолжения или повторного запуска обхода. \n```python\n# settings.py\n\nHTTPCACHE_ENABLED = True\nHTTPCACHE_EXPIRATION_SECS = 0\nHTTPCACHE_DIR = \"httpcache\"\nHTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n```\n\n2. **Управление Интенсивностью Обхода**: более щадящий режим работы скрапера, чтобы минимизировать нагрузку на веб-сайты, уважая эти ресурсы и не нарушая их работу. \nНу и избежание потенциальных блокировок со стороны веб-сайтов.\n```python\n# settings.py\n\nDOWNLOAD_DELAY = 0.25\nCONCURRENT_REQUESTS_PER_DOMAIN = 4\n\nAUTOTHROTTLE_ENABLED = True\nAUTOTHROTTLE_START_DELAY = 5\nAUTOTHROTTLE_MAX_DELAY = 60\nAUTOTHROTTLE_DEBUG = True\n```\n\n\n### **Примеры Кода**\n\nПример кода паука Scrapy, который был использован для сбора данных с сайта [apkbr.ru](http://www.apkbr.ru/):\n\n```python\nimport os\n\nfrom scrapy.exporters import JsonLinesItemExporter\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\n\n\nclass ApkbrRuSpider(CrawlSpider):\n    name = 'apkbr_ru'\n    allowed_domains = ['apkbr.ru']\n    start_urls = ['https://apkbr.ru/']\n\n    rules = (\n        Rule(LinkExtractor(allow=r'/node?page=\\d+'), follow=True),\n        Rule(LinkExtractor(allow=r'/calendar/.+'), follow=True),\n        Rule(LinkExtractor(allow=r'/node/\\d+'), callback='parse_item', follow=True),\n    )\n\n    def __init__(self, *args, **kwargs):\n        super(ApkbrRuSpider, self).__init__(*args, **kwargs)\n        self.dump_dir = f'../data/{self.name}'\n        os.makedirs(self.dump_dir, exist_ok=True)\n\n        self.file = open(os.path.join(self.dump_dir, 'apkbr_ru.jl'), 'wb')\n        self.exporter = JsonLinesItemExporter(self.file, encoding='utf-8', ensure_ascii=False)\n        self.exporter.start_exporting()\n\n    def close_spider(self, spider):\n        self.exporter.finish_exporting()\n        self.file.close()\n\n    def parse_item(self, response):\n        title = response.css('h1.title::text').get()\n        publication_date = response.css('div.meta.submitted span::attr(content)').get()\n        content = ''.join(response.css('div.field-name-body ::text').getall())\n        author = response.css('div.field-name-field-author .field-item::text').get()\n\n        item = {\n            'url': response.url,\n            'title': title.strip() if title else None,\n            'publication_date': publication_date.strip() if publication_date else None,\n            'content': content.strip() if content else None,\n            'author': author.strip() if author else None,\n        }\n\n        self.exporter.export_item(item)\n        return item\n\n```\n\n1. **Настройка и Инициализация**: Создание директории для данных, открытие файла для сохранения данных в формате JSON Lines.\n\n2. **Правила Обхода**: Определение правил для LinkExtractor, чтобы указать, какие ссылки следует посещать.\n\n```python\nrules = (\n    Rule(LinkExtractor(allow=r'/node?page=\\d+'), follow=True),  # обход пагинации\n    Rule(LinkExtractor(allow=r'/calendar/.+'), follow=True),  # обход календаря\n    Rule(LinkExtractor(allow=r'/node/\\d+'), callback='parse_item', follow=True),  # обход и извлечение данных\n)\n```\nНам не нужны парсить все страницы, поэтому мы ограничиваемся только теми, которые содержат новости.\nЭто страницы с ссылками вида `/node/1234`.\nНо обход пагинации и календаря также важен, поскольку он позволяет не пропустить страницы с новостями.  \n\n3. **Парсинг Страниц**: Функция `parse_item` извлекает необходимые данные с веб-страницы, включая заголовок, дату публикации, содержимое и автора.\n\n4. **Экспорт Данных**: Использование `JsonLinesItemExporter` для структурирования и сохранения данных в удобочитаемом формате.\n\nВажно собрать структурированные данные, чтобы облегчить их дальнейшее использование и анализ.\n\n### **Структура данных**\n\nСобранные данные были сохранены в формате JSON Lines, который представляет собой последовательность строк, каждая из которых содержит отдельный JSON-объект. Это удобный формат для хранения структурированных данных, поскольку он позволяет легко извлекать и использовать данные.\n\nПример структуры данных:\n```json\n{\n    \"url\": \"https://apkbr.ru/node/1234\",\n    \"title\": \"Заголовок Новости\",\n    \"publication_date\": \"2021-01-01T00:00:00+03:00\",\n    \"content\": \"Текст Новости\",\n    \"author\": \"Автор Новости\"\n}\n```\n\nНаибольший интерес представляет поле `content`, которое содержит текст новостей и статей. \nЭто основной материал для дальнейшего анализа и исследований.\nНо остальные поля также могут быть полезны для дополнительного анализа или использования в качестве метаданных.\n\n\n### **Ссылка на Репозиторий проекта для сбора данных**\n- **Github проекта**: [zbze_crawler](https://github.com/panagoa/zbze_crawler/)\n- **Использование проекта**: \n    - Склонируйте репозиторий: `git clone git@github.com:panagoa/zbze_crawler.git`\n    - Установите зависимости: `pip install -r requirements.in`\n    - Запустите паука: `scrapy crawl apkbr_ru` из директории `zbze_scrapy`\n- **Структура собранных данных**:\n```\ndata\n├── apkbr_ru\n│   ├── apkbr_ru.jl\n├── apkbr_ru_rss\n│   ├── apkbr_ru_rss.jl\n├── elgkbr_ru\n│   ├── elgkbr_ru.jl\n└── oshhamaho\n    ├── 01-2011.pdf\n    ├── 01-2013.pdf\n```\n\n### **Заключение**\n\nС этой статьей мы начинаем важное путешествие по сохранению и развитию кабардинского языка в цифровую эпоху. \nМы не просто рассмотрели базовый процесс сбора данных, но и открыли дверь к новым возможностям и подходам в исследовании малоресурсных языков. Это исследование — не просто техническая задача, но и культурный проект, который помогает сохранить и обогатить уникальное языковое наследие.\n\nЯ приглашаю вас углубиться в тему сбора и анализа данных с помощью дополнительных материалов, доступных в разделе [Ссылки на Дополнительные Материалы](#ссылки-на-дополнительные-материалы). \nВ следующих статьях мы продолжим наше путешествие, сосредоточив внимание на анализе собранных данных и извлечении из них полезной информации. \nВместе мы сможем не только сохранить, но и обогатить кабардинский язык, давая ему новую жизнь в современном мире.\n\n\n### **Как Вы Можете Помочь**\n\nВаш активный вклад и обратная связь играют ключевую роль в повышении качества и доступности данных для исследований и разработки. \n\n- **Улучшение кода**: Если у вас есть опыт в программировании, помогите улучшить код нашего проекта, предлагая исправления или новые функции. Ваш технический вклад поможет сделать наш инструмент более мощным и доступным для других исследователей.\n\n- **Знание кабардинского языка и культуры**: Если вы знакомы с кабардинским языком или культурой, поделитесь идеями по расширению нашей базы данных или улучшению точности сбора данных. Ваше знание языка и культуры является ценным ресурсом для обогащения нашего проекта.\n\nКаждый ваш вклад, будь то маленький совет или большой кусок кода, приближает нас к сохранению кабардинского языка и его интеграции в современный цифровой мир. Работая вместе, мы не только сохраняем языковое наследие, но и создаем новые возможности для его изучения и использования в будущем.\n\n### Ссылки на Дополнительные Материалы\n- [Scrapy documentation](https://docs.scrapy.org/en/latest/)\n- [Introducing Scrapy: The Powerful Python Library For Efficient Web Scraping](https://medium.com/aws-tip/introducing-scrapy-the-powerful-python-library-for-efficient-web-scraping-ef4557ec6d2)\n- [How-to: Build a Python Web Scraper to capture IMDb Top-100 Movies](https://medium.com/@abdulrwahab/how-to-build-a-python-web-scraper-to-capture-imdb-top-100-movies-908bf9b6bc19)\n- [awesome-web-scraping](https://github.com/lorien/awesome-web-scraping?tab=readme-ov-file)",
    "title": "\"Сбор данных для исследования кабардинского языка\"",
    "summary": "Статья посвящена проблематике исчезновения малоресурсных языков в условиях цифровой эры и рассматривает способы их сохранения на примере сбора и анализа данных на кабардинском языке из цифровых ресурсов. В статье подробно рассказывается о использовании инструмента Scrapy для веб-скрапинга данных, представлены основные этапы настройки, примеры кода, структура собранных данных и ссылки на репозиторий проекта."
  },
  "../blog/templates/index.tmpl.md": {
    "filename": "index.tmpl.md",
    "path": "../blog/templates/index.tmpl.md",
    "relative_path": "templates/index.tmpl.md",
    "hash": "b5233e64849c7cf351f5e96c76eaf5ea",
    "is_updated": false,
    "content": "# Индекс блога\n\n{% if base %}\n## Base\nЭта папка содержит основные материалы, вводные статьи про проект и общую информацию.\n{% for file_info in base %}\n- [{{ file_info.title or 'Без названия' }}](../{{ file_info.relative_path }}) : {{ file_info.summary }}\n{% endfor %}\n{% endif %}\n\n{% if articles %}\n## Articles\nПапка включает в себя статьи, подробно описывающие шаги проекта. Каждый этап проекта оформлен в виде отдельной статьи.\n{% for file_info in articles %}\n- [{{ file_info.title or 'Без названия' }}](../{{ file_info.relative_path }}) : {{ file_info.summary }}\n{% endfor %}\n{% endif %}\n\n\n## Snippets\nЭтот раздел включает сниппеты, кои подробно объясняют код из использованных файлов и scripts.\n\n{% if notebooks_snippets %}\n### Auto Generated from Notebooks\n{% for file_info in notebooks_snippets %}\n- [{{ file_info.title or 'Без названия' }}](../{{ file_info.relative_path }}) : {{ file_info.summary }}\n{% endfor %}\n{% endif %}\n\n{% if python_snippets %}\n### Auto Generated from Python\n{% for file_info in python_snippets %}\n- [{{ file_info.title or 'Без названия' }}](../{{ file_info.relative_path }}) : {{ file_info.summary }}\n{% endfor %}\n{% endif %}\n\n\n\n\n\n",
    "title": "# Структура и содержание блога",
    "summary": "Markdown файл '../blog/templates/index.tmpl.md' служит шаблоном для создания индексной страницы блога. В нем представлены разделы для вводных материалов, статей, описывающих этапы проекта, а также раздел сниппетов, сгенерированных из рабочих тетрадей и Python скриптов."
  }
}