# Индекс блога


## Base
Раздел содержит основные материалы, вводные статьи про проект и общую информацию.

- ["Обзор и значимость кабардинского языка"](./00_base/01_about_kabardian_language.md) : Статья предлагает глубокий обзор кабардинского языка, анализируя его историю, уникальные характеристики и значение для современного мира, указывая на необходимость его сохранения и изучения.


- ["Мотивация и цели создания блога по NLP"](./00_base/02_project_motivation_and_goals.md) : Файл содержит описание личной мотивации автора для создания блога, целях данного проекта и его потенциальное значение в области обработки естественного языка. В нем также обсуждаются причины изучения кабардинского языка и ожидаемые результаты этого проекта.

- ["Методы сбора данных для анализа кабардинского языка"](./00_base/03_data_collection_and_sources.md) : Страница описывает различные методы и стратегии сбора данных для анализа кабардинского языка, включая подробное рассмотрение таких источников данных, как онлайн-публикации, архивы и цифровые библиотеки.

- ["Технические аспекты обработки и структурирования данных"](./00_base/04_data_processing_and_structure.md) : Файл освещает технические аспекты работы со собранными данными, а именно: их очистку, структурирование и подготовку для дальнейшего анализа, при этом рассматриваются используемые инструменты и технологии.

- ["Юридические аспекты использования данных"](./00_base/05_copyrights_and_usage_terms.md) : Страница посвящена юридическим нюансам использования данных, где обсуждаются вопросы авторских прав, условий использования контента и связанных с ним лицензий.

- ["Возможности проекта для научных исследований"](./00_base/06_research_opportunities.md) : Этот файл содержит обсуждение возможностей проекта в контексте научных исследований, особенно для лингвистов, исследователей и разработчиков, работающих с кабардинским языком и областью обработки естественного языка (NLP).

- ["Контакты и приглашение к сотрудничеству"](./00_base/07_contacts_and_collaboration.md) : Этот файл содержит информацию о том, как связаться с автором блога, а также предлагает возможность сотрудничества, обмена знаниями и участия в проекте.

- [[Будущее и развитие проекта](../blog/00_base/08_future_of_the_project.md)](./00_base/08_future_of_the_project.md) : Этот файл содержит обсуждение планов и стратегий для будущего развития проекта, включая углубление исследований и создание новых инструментов и методов.




## Articles
Раздел включает в себя статьи, подробно описывающие шаги проекта. Каждый этап проекта оформлен в виде отдельной статьи.

- ["Сбор данных для исследования кабардинского языка"](./01_articles/01_kabardian_language_data_collection.md) : Статья посвящена проблематике исчезновения малоресурсных языков в условиях цифровой эры и рассматривает способы их сохранения на примере сбора и анализа данных на кабардинском языке из цифровых ресурсов. В статье подробно рассказывается о использовании инструмента Scrapy для веб-скрапинга данных, представлены основные этапы настройки, примеры кода, структура собранных данных и ссылки на репозиторий проекта.




## Snippets
Раздел включает сниппеты, кои подробно объясняют код из использованных файлов и scripts.


### Auto Generated from Notebooks
Раздел включает сниппеты, сгенерированные автоматически из Jupyter Notebook файлов из директории `notebooks`

- ["Очистка и сохранение веб-скрейпинг данных"](./02_snippets/auto_generated/notebooks/00_clean_scraped_text.md) : Файл '00_clean_scraped_text.md' - это подробное руководство по очистке и предобработке текстовых данных, полученных с помощью web-скрапинга. С использованием различных библиотек Python и созданных функций, автор обрабатывает json-файлы, считывает и очищает текст от нежелательных символов, затем сохраняет обработанные данные в форматы .json и .txt для дальнейшего анализа.

- ["Извлечение текста из PDF файлов с использованием Python"](./02_snippets/auto_generated/notebooks/01_extract_text_from_pdf.md) : Статья описывает процесс извлечения текста из PDF файлов с использованием Python и модулей `os`, `tqdm` и `textract`. Описываются шаги создания репозитория для хранения текстовых файлов, определение функции для извлечения текста, применение функции для обработки всех PDF-файлов в репозитории и последующая очистка текста.

- [# Извлечение и анализ биграмм из текста](./02_snippets/auto_generated/notebooks/02_00_text_analyze_bigram_extract.md) : Этот Markdown файл представляет собой инструкцию по анализу текста и извлечению биграмм с использованием Python и библиотеки NLTK. Объясняется процесс чтения данных из файла, извлечения биграмм, создания DataFrame для удобства обработки, сохранения полученных данных и расчета вероятности специфической последовательности слов в тексте.

- [# Создание частотного словаря слов с Python](./02_snippets/auto_generated/notebooks/02_01_text_analyze_word_extract.md) : Этот Markdown файл представляет собой руководство по анализу текста и извлечению ключевых слов с использованием Python. В нем объясняется, как создать частотный словарь слов из выбранных текстовых файлов с помощью библиотек `collections`, `nltk`, `pandas` и `os`.

- [# Сравнительный анализ частотности слов по источникам](./02_snippets/auto_generated/notebooks/02_01_text_analyze_word_frequent_compare_by_source.md) : 'Анализ частоты слов сравнение по источникам' - файл, иллюстрирующий анализ частотности слов в текстах из различных источников. Описывает процесс загрузки и предобработки данных, расчёт топа самых часто встречающихся слов и уникальных слов, с особым акцентом на определение характерных особенностей каждого источника.

- ["Создание токенизатора с использованием BPE"](./02_snippets/auto_generated/notebooks/02_02_text_analyze_tokenizer_bpe.md) : Статья описывает процесс создания и обучения собственного токенизатора на основе подхода Byte Pair Encoding (BPE). Приводится код и пошаговое пояснение, от обучения до его использования и сохранения. Эта информация поможет в создании настроенных токенизаторов для работы с специфическими текстовыми данными.

- [# Токенизация текста с помощью метода Unigram
](./02_snippets/auto_generated/notebooks/02_03_text_analyze_tokenizer_unigram.md) : Статья посвящена процессу создания, обучения и проверки работы Unigram токенизатора для разбиения текстовых файлов на отдельные элементы - токены. В статье подробно разбирается использование нужных библиотек, добавление кастомных токенов, настройка параметров обучения и сохранение готового токенизатора в файл.




### Auto Generated from Python
Раздел включает сниппеты, сгенерированные автоматически из Python файлов из директории `src`

- ["Создание HTTP-клиента для взаимодействия с OpenAI API"](./02_snippets/auto_generated/python/openai.md) : Этот Markdown файл описывает процесс создания HTTP-клиента с использованием библиотеки OpenAI для взаимодействия с OpenAI API. Внутри приведён пример кода с подробным объяснением работы методов и классов, включая обработку исключений и последующую реализацию HTTP-клиента.

- ["Автоматическая генерация сниппетов кода Python"](./02_snippets/auto_generated/python/snippet_generator.md) : Файл 'snippet_generator.md' содержит подробное объяснение работы Python скрипта, предназначенного для автоматического создания кратких представлений (snippets) кода в формате Markdown с использованием модели OpenAI gpt-4. В статье детально рассмотрены основные части кода, их функции и принцип работы.

- ["Очистка текста в Python: подробное руководство"](./02_snippets/auto_generated/python/text_cleaner.md) : Markdown файл '../blog/02_snippets/auto_generated/python/text_cleaner.md' содержит руководство по очистке и предобработке текстовых данных на языке Python. Описываются методы удаления лишних символов, повторяющихся единиц текста и "мусорного" текста, замены слов, объединение слов, разделённых дефисами. Подробно разбираются функции препроцессинга и демонстрируется пример их работы.

- ["Генерация индекса блога с Python и OpenAI"](./02_snippets/auto_generated/python/update_index_with_summaries.md) : Этот Markdown-файл представляет собой руководство по созданию индексной страницы блога с использованием Python и Open AI. В статье описывается процесс чтения файлов статей блога, генерации заголовка и краткого описания для каждого файла с помощью GPT-4, и последующего создания индексной страницы со ссылками на все статьи.

