# Анализ текста и извлечение слов с Python 

В данном сниппете используется язык программирования Python для создания частотного словаря слов на основе выбранных текстовых файлов. Иными словами, мы просмотрим выбранные текстовые файлы, распарсим их на отдельные слова, посчитаем частотность каждого слова и записим результаты в новый файл. 

Библиотеки, которые нам понадобятся:
- `collections` - для использования класса `Counter`, который упростит подсчет частоты появления слов.
- `nltk` - для разбиения текста на слова (токенов).
- `pandas` - для создания и манипуляции с данными в формате таблиц.
- `os` - для работы с файловой системой.

### Объяснение кода

**1. Импорт библиотек и установка параметров**

В начале импортируются все необходимые библиотеки. Включается кнопка скачивания стоп-слов для nltk. Устанавливаются пути до директорий с входными и выходными данными. Создаётся счетчик слов, который будет общим для всех файлов.

```python
from collections import Counter
import os
import nltk
import pandas as pd

nltk.download('stopwords')

input_dir = '../data/processed'
output_dir = '../data/processed/word_freqs'

os.makedirs(output_dir, exist_ok=True)

BUF_SIZE = 100000
word_limit = 1000000

all_word_counter = Counter()
```
**2. Обработка файлов**

Затем идет описание двух функций, которые обрабатывают счетчики слов - `save_freqs_flat` и `save_with_freq`. Первая функция просто сохраняет слова отсортированными по частоте встречаемости, вторая сохраняет слова вместе с частотой, при этом можно настроить ограничение на минимальную частоту, которая нужна для сохранения в файл.

```python
def save_freqs_flat(counter, output_path):
    # ...

def save_with_freq(counter, output_path, freq_limit=0):
    # ...
```

Затем идёт цикл обработки входных файлов. Для каждого файла открывается поток чтения, данные читаются блоками заданного размера, токенизируются с помощью nltk и эти слова передаются в соответствующие счётчики - общий и отдельный для каждого файла. Результаты сохраняются в два файла для каждого исходного.

```python
for input_file_name in ['oshhamaho.txt', 'apkbr_ru.txt', 'elgkbr_ru.txt']:
    # ...
```

Этот код - простой, но мощный инструмент для начального анализа текста, который может помочь в поиске ключевых слов и выявлении общих тем в текстовых данных.